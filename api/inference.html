
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Inference &#8212; PyMC3 3.11.0 documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../_static/highlight.min.js"></script>
    <script src="../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-176578023-1']);
  _gaq.push(['_trackPageview']);
</script>
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../nb_examples/index.html" class="item">Examples</a> <a href="../learn.html" class="item">Books + Videos</a> <a href="../api.html" class="item">API</a> <a href="../developer_guide.html" class="item">Developer Guide</a> <a href="../about.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  <div class="section" id="inference">
<h1>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pymc3.sampling">
<span id="sampling"></span><h2>Sampling<a class="headerlink" href="#module-pymc3.sampling" title="Permalink to this headline">¶</a></h2>
<p>Functions for MCMC sampling.</p>
<dl class="py function">
<dt id="pymc3.sampling.fast_sample_posterior_predictive">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">fast_sample_posterior_predictive</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">trace</span><span class="p">:</span> <span class="n">MultiTrace | Dataset | InferenceData | list[dict[str, np.ndarray]]</span></em>, <em class="sig-param"><span class="n">samples</span><span class="p">:</span> <span class="n">int | None</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Model | None</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">list[str] | None</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keep_size</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; dict<span class="p">[</span>str<span class="p">, </span>np.ndarray<span class="p">]</span><a class="headerlink" href="#pymc3.sampling.fast_sample_posterior_predictive" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate posterior predictive samples from a model given a trace.</p>
<p>This is a vectorized alternative to the standard <code class="docutils literal notranslate"><span class="pre">sample_posterior_predictive</span></code> function.
It aims to be as compatible as possible with the original API, and is significantly
faster.  Both posterior predictive sampling functions have some remaining issues, and
we encourage users to verify agreement across the results of both functions for the time
being.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>trace: MultiTrace, xarray.Dataset, InferenceData, or List of points (dictionary)</strong></dt><dd><p>Trace generated from MCMC sampling.</p>
</dd>
<dt><strong>samples: int, optional</strong></dt><dd><p>Number of posterior predictive samples to generate. Defaults to one posterior predictive
sample per posterior sample, that is, the number of draws times the number of chains. It
is not recommended to modify this value; when modified, some chains may not be represented
in the posterior predictive sample.</p>
</dd>
<dt><strong>model: Model (optional if in `with` context)</strong></dt><dd><p>Model used to generate <cite>trace</cite></p>
</dd>
<dt><strong>var_names: Iterable[str]</strong></dt><dd><p>List of vars to sample.</p>
</dd>
<dt><strong>keep_size: bool, optional</strong></dt><dd><p>Force posterior predictive sample to have the same shape as posterior and sample stats
data: <code class="docutils literal notranslate"><span class="pre">(nchains,</span> <span class="pre">ndraws,</span> <span class="pre">...)</span></code>.</p>
</dd>
<dt><strong>random_seed: int</strong></dt><dd><p>Seed for the random number generator.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>samples: dict</dt><dd><p>Dictionary with the variable names as keys, and values numpy arrays containing
posterior predictive samples.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.init_nuts">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">init_nuts</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">chains</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">n_init</span><span class="o">=</span><span class="default_value">500000</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">jitter_max_retries</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.init_nuts" title="Permalink to this definition">¶</a></dt>
<dd><p>Set up the mass matrix initialization for NUTS.</p>
<p>NUTS convergence and sampling speed is extremely dependent on the
choice of mass/scaling matrix. This function implements different
methods for choosing or adapting the mass matrix.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>init</strong><span class="classifier">str</span></dt><dd><p>Initialization method to use.</p>
<ul class="simple">
<li><p>auto: Choose a default initialization method automatically.
Currently, this is <code class="docutils literal notranslate"><span class="pre">jitter+adapt_diag</span></code>, but this can change in the future. If you
depend on the exact behaviour, choose an initialization method explicitly.</p></li>
<li><p>adapt_diag: Start with a identity mass matrix and then adapt a diagonal based on the
variance of the tuning samples. All chains use the test value (usually the prior mean)
as starting point.</p></li>
<li><p>jitter+adapt_diag: Same as <code class="docutils literal notranslate"><span class="pre">adapt_diag</span></code>, but use test value plus a uniform jitter in
[-1, 1] as starting point in each chain.</p></li>
<li><p>advi+adapt_diag: Run ADVI and then adapt the resulting diagonal mass matrix based on the
sample variance of the tuning samples.</p></li>
<li><p>advi+adapt_diag_grad: Run ADVI and then adapt the resulting diagonal mass matrix based
on the variance of the gradients during tuning. This is <strong>experimental</strong> and might be
removed in a future release.</p></li>
<li><p>advi: Run ADVI to estimate posterior mean and diagonal mass matrix.</p></li>
<li><p>advi_map: Initialize ADVI with MAP and use MAP as starting point.</p></li>
<li><p>map: Use the MAP as starting point. This is discouraged.</p></li>
<li><p>adapt_full: Adapt a dense mass matrix using the sample covariances. All chains use the
test value (usually the prior mean) as starting point.</p></li>
<li><p>jitter+adapt_full: Same as <code class="docutils literal notranslate"><span class="pre">adapt_full</span></code>, but use test value plus a uniform jitter in
[-1, 1] as starting point in each chain.</p></li>
</ul>
</dd>
<dt><strong>chains</strong><span class="classifier">int</span></dt><dd><p>Number of jobs to start.</p>
</dd>
<dt><strong>n_init</strong><span class="classifier">int</span></dt><dd><p>Number of iterations of initializer. Only works for ‘ADVI’ init methods.</p>
</dd>
<dt><strong>model</strong><span class="classifier">Model (optional if in <code class="docutils literal notranslate"><span class="pre">with</span></code> context)</span></dt><dd></dd>
<dt><strong>progressbar</strong><span class="classifier">bool</span></dt><dd><p>Whether or not to display a progressbar for advi sampling.</p>
</dd>
<dt><strong>jitter_max_retries</strong><span class="classifier">int</span></dt><dd><p>Maximum number of repeated attempts (per chain) at creating an initial matrix with uniform jitter
that yields a finite probability. This applies to <code class="docutils literal notranslate"><span class="pre">jitter+adapt_diag</span></code> and <code class="docutils literal notranslate"><span class="pre">jitter+adapt_full</span></code>
init methods.</p>
</dd>
<dt><strong>**kwargs</strong><span class="classifier">keyword arguments</span></dt><dd><p>Extra keyword arguments are forwarded to pymc3.NUTS.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>start</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">pymc3.model.Point</span></code></span></dt><dd><p>Starting point for sampler</p>
</dd>
<dt><strong>nuts_sampler</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">pymc3.step_methods.NUTS</span></code></span></dt><dd><p>Instantiated and initialized NUTS sampler object</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.iter_sample">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">iter_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">draws</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step</span></em>, <em class="sig-param"><span class="n">start</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">chain</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">tune</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="model.html#pymc3.model.Model" title="pymc3.model.Model">pymc3.model.Model</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">callback</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.iter_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a trace on each iteration using the given step method.</p>
<p>Multiple step methods ared supported via compound step methods.  Returns the
amount of time taken.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>draws</strong><span class="classifier">int</span></dt><dd><p>The number of samples to draw</p>
</dd>
<dt><strong>step</strong><span class="classifier">function</span></dt><dd><p>Step function</p>
</dd>
<dt><strong>start</strong><span class="classifier">dict</span></dt><dd><p>Starting point in parameter space (or partial point). Defaults to trace.point(-1)) if
there is a trace provided and model.test_point if not (defaults to empty dict)</p>
</dd>
<dt><strong>trace</strong><span class="classifier">backend, list, or MultiTrace</span></dt><dd><p>This should be a backend instance, a list of variables to track, or a MultiTrace object
with past values. If a MultiTrace object is given, it must contain samples for the chain
number <code class="docutils literal notranslate"><span class="pre">chain</span></code>. If None or a list of variables, the NDArray backend is used.</p>
</dd>
<dt><strong>chain</strong><span class="classifier">int, optional</span></dt><dd><p>Chain number used to store sample in backend. If <code class="docutils literal notranslate"><span class="pre">cores</span></code> is greater than one, chain numbers
will start here.</p>
</dd>
<dt><strong>tune</strong><span class="classifier">int, optional</span></dt><dd><p>Number of iterations to tune, if applicable (defaults to None)</p>
</dd>
<dt><strong>model</strong><span class="classifier">Model (optional if in <code class="docutils literal notranslate"><span class="pre">with</span></code> context)</span></dt><dd></dd>
<dt><strong>random_seed</strong><span class="classifier">int or list of ints, optional</span></dt><dd><p>A list is accepted if more if <code class="docutils literal notranslate"><span class="pre">cores</span></code> is greater than one.</p>
</dd>
<dt><strong>callback :</strong></dt><dd><p>A function which gets called for every sample from the trace of a chain. The function is
called with the trace and the current draw and will contain all samples for a single trace.
the <code class="docutils literal notranslate"><span class="pre">draw.chain</span></code> argument can be used to determine which of the active chains the sample
is drawn from.
Sampling can be interrupted by throwing a <code class="docutils literal notranslate"><span class="pre">KeyboardInterrupt</span></code> in the callback.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>trace</strong><span class="classifier">MultiTrace</span></dt><dd><p>Contains all samples up to the current iteration</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">trace</span> <span class="ow">in</span> <span class="n">iter_sample</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.sample">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">draws</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">step</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">n_init</span><span class="o">=</span><span class="default_value">200000</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">chain_idx</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">chains</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cores</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tune</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">discard_tuned_samples</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">compute_convergence_checks</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">callback</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">jitter_max_retries</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">return_inferencedata</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">idata_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mp_ctx</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pickle_backend</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'pickle'</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from the posterior using the given step methods.</p>
<p>Multiple step methods are supported via compound step methods.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>draws</strong><span class="classifier">int</span></dt><dd><p>The number of samples to draw. Defaults to 1000. The number of tuned samples are discarded
by default. See <code class="docutils literal notranslate"><span class="pre">discard_tuned_samples</span></code>.</p>
</dd>
<dt><strong>init</strong><span class="classifier">str</span></dt><dd><p>Initialization method to use for auto-assigned NUTS samplers.</p>
<ul class="simple">
<li><p>auto: Choose a default initialization method automatically.
Currently, this is <code class="docutils literal notranslate"><span class="pre">jitter+adapt_diag</span></code>, but this can change in the future.
If you depend on the exact behaviour, choose an initialization method explicitly.</p></li>
<li><p>adapt_diag: Start with a identity mass matrix and then adapt a diagonal based on the
variance of the tuning samples. All chains use the test value (usually the prior mean)
as starting point.</p></li>
<li><p>jitter+adapt_diag: Same as <code class="docutils literal notranslate"><span class="pre">adapt_diag</span></code>, but add uniform jitter in [-1, 1] to the
starting point in each chain.</p></li>
<li><p>advi+adapt_diag: Run ADVI and then adapt the resulting diagonal mass matrix based on the
sample variance of the tuning samples.</p></li>
<li><p>advi+adapt_diag_grad: Run ADVI and then adapt the resulting diagonal mass matrix based
on the variance of the gradients during tuning. This is <strong>experimental</strong> and might be
removed in a future release.</p></li>
<li><p>advi: Run ADVI to estimate posterior mean and diagonal mass matrix.</p></li>
<li><p>advi_map: Initialize ADVI with MAP and use MAP as starting point.</p></li>
<li><p>map: Use the MAP as starting point. This is discouraged.</p></li>
<li><p>adapt_full: Adapt a dense mass matrix using the sample covariances</p></li>
</ul>
</dd>
<dt><strong>step</strong><span class="classifier">function or iterable of functions</span></dt><dd><p>A step function or collection of functions. If there are variables without step methods,
step methods for those variables will be assigned automatically.  By default the NUTS step
method will be used, if appropriate to the model; this is a good default for beginning
users.</p>
</dd>
<dt><strong>n_init</strong><span class="classifier">int</span></dt><dd><p>Number of iterations of initializer. Only works for ‘ADVI’ init methods.</p>
</dd>
<dt><strong>start</strong><span class="classifier">dict, or array of dict</span></dt><dd><p>Starting point in parameter space (or partial point)
Defaults to <code class="docutils literal notranslate"><span class="pre">trace.point(-1))</span></code> if there is a trace provided and model.test_point if not
(defaults to empty dict). Initialization methods for NUTS (see <code class="docutils literal notranslate"><span class="pre">init</span></code> keyword) can
overwrite the default.</p>
</dd>
<dt><strong>trace</strong><span class="classifier">backend, list, or MultiTrace</span></dt><dd><p>This should be a backend instance, a list of variables to track, or a MultiTrace object
with past values. If a MultiTrace object is given, it must contain samples for the chain
number <code class="docutils literal notranslate"><span class="pre">chain</span></code>. If None or a list of variables, the NDArray backend is used.</p>
</dd>
<dt><strong>chain_idx</strong><span class="classifier">int</span></dt><dd><p>Chain number used to store sample in backend. If <code class="docutils literal notranslate"><span class="pre">chains</span></code> is greater than one, chain
numbers will start here.</p>
</dd>
<dt><strong>chains</strong><span class="classifier">int</span></dt><dd><p>The number of chains to sample. Running independent chains is important for some
convergence statistics and can also reveal multiple modes in the posterior. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
then set to either <code class="docutils literal notranslate"><span class="pre">cores</span></code> or 2, whichever is larger.</p>
</dd>
<dt><strong>cores</strong><span class="classifier">int</span></dt><dd><p>The number of chains to run in parallel. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, set to the number of CPUs in the
system, but at most 4.</p>
</dd>
<dt><strong>tune</strong><span class="classifier">int</span></dt><dd><p>Number of iterations to tune, defaults to 1000. Samplers adjust the step sizes, scalings or
similar during tuning. Tuning samples will be drawn in addition to the number specified in
the <code class="docutils literal notranslate"><span class="pre">draws</span></code> argument, and will be discarded unless <code class="docutils literal notranslate"><span class="pre">discard_tuned_samples</span></code> is set to
False.</p>
</dd>
<dt><strong>progressbar</strong><span class="classifier">bool, optional default=True</span></dt><dd><p>Whether or not to display a progress bar in the command line. The bar shows the percentage
of completion, the sampling speed in samples per second (SPS), and the estimated remaining
time until completion (“expected time of arrival”; ETA).</p>
</dd>
<dt><strong>model</strong><span class="classifier">Model (optional if in <code class="docutils literal notranslate"><span class="pre">with</span></code> context)</span></dt><dd></dd>
<dt><strong>random_seed</strong><span class="classifier">int or list of ints</span></dt><dd><p>A list is accepted if <code class="docutils literal notranslate"><span class="pre">cores</span></code> is greater than one.</p>
</dd>
<dt><strong>discard_tuned_samples</strong><span class="classifier">bool</span></dt><dd><p>Whether to discard posterior samples of the tune interval.</p>
</dd>
<dt><strong>compute_convergence_checks</strong><span class="classifier">bool, default=True</span></dt><dd><p>Whether to compute sampler statistics like Gelman-Rubin and <code class="docutils literal notranslate"><span class="pre">effective_n</span></code>.</p>
</dd>
<dt><strong>callback</strong><span class="classifier">function, default=None</span></dt><dd><p>A function which gets called for every sample from the trace of a chain. The function is
called with the trace and the current draw and will contain all samples for a single trace.
the <code class="docutils literal notranslate"><span class="pre">draw.chain</span></code> argument can be used to determine which of the active chains the sample
is drawn from.
Sampling can be interrupted by throwing a <code class="docutils literal notranslate"><span class="pre">KeyboardInterrupt</span></code> in the callback.</p>
</dd>
<dt><strong>jitter_max_retries</strong><span class="classifier">int</span></dt><dd><p>Maximum number of repeated attempts (per chain) at creating an initial matrix with uniform jitter
that yields a finite probability. This applies to <code class="docutils literal notranslate"><span class="pre">jitter+adapt_diag</span></code> and <code class="docutils literal notranslate"><span class="pre">jitter+adapt_full</span></code>
init methods.</p>
</dd>
<dt><strong>return_inferencedata</strong><span class="classifier">bool, default=False</span></dt><dd><p>Whether to return the trace as an <a class="reference external" href="https://arviz-devs.github.io/arviz/api/generated/arviz.InferenceData.html#arviz.InferenceData" title="(in ArviZ vdev)"><code class="docutils literal notranslate"><span class="pre">arviz.InferenceData</span></code></a> (True) object or a <cite>MultiTrace</cite> (False)
Defaults to <cite>False</cite>, but we’ll switch to <cite>True</cite> in an upcoming release.</p>
</dd>
<dt><strong>idata_kwargs</strong><span class="classifier">dict, optional</span></dt><dd><p>Keyword arguments for <a class="reference external" href="https://arviz-devs.github.io/arviz/api/generated/arviz.from_pymc3.html#arviz.from_pymc3" title="(in ArviZ vdev)"><code class="docutils literal notranslate"><span class="pre">arviz.from_pymc3()</span></code></a></p>
</dd>
<dt><strong>mp_ctx</strong><span class="classifier">multiprocessing.context.BaseContent</span></dt><dd><p>A multiprocessing context for parallel sampling. See multiprocessing
documentation for details.</p>
</dd>
<dt><strong>pickle_backend</strong><span class="classifier">str</span></dt><dd><p>One of <cite>‘pickle’</cite> or <cite>‘dill’</cite>. The library used to pickle models
in parallel sampling if the multiprocessing context is not of type
<cite>fork</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>trace</strong><span class="classifier">pymc3.backends.base.MultiTrace or arviz.InferenceData</span></dt><dd><p>A <code class="docutils literal notranslate"><span class="pre">MultiTrace</span></code> or ArviZ <code class="docutils literal notranslate"><span class="pre">InferenceData</span></code> object that contains the samples.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Optional keyword arguments can be passed to <code class="docutils literal notranslate"><span class="pre">sample</span></code> to be delivered to the
<code class="docutils literal notranslate"><span class="pre">step_method</span></code>s used during sampling.</p>
<p>If your model uses only one step method, you can address step method kwargs
directly. In particular, the NUTS step method has several options including:</p>
<blockquote>
<div><ul class="simple">
<li><p>target_accept : float in [0, 1]. The step size is tuned such that we
approximate this acceptance rate. Higher values like 0.9 or 0.95 often
work better for problematic posteriors</p></li>
<li><p>max_treedepth : The maximum depth of the trajectory tree</p></li>
<li><p>step_scale : float, default 0.25
The initial guess for the step size scaled down by <span class="math notranslate nohighlight">\(1/n**(1/4)\)</span></p></li>
</ul>
</div></blockquote>
<p>If your model uses multiple step methods, aka a Compound Step, then you have
two ways to address arguments to each step method:</p>
<ol class="upperalpha">
<li><p>If you let <code class="docutils literal notranslate"><span class="pre">sample()</span></code> automatically assign the <code class="docutils literal notranslate"><span class="pre">step_method</span></code>s,
and you can correctly anticipate what they will be, then you can wrap
step method kwargs in a dict and pass that to sample() with a kwarg set
to the name of the step method.
e.g. for a CompoundStep comprising NUTS and BinaryGibbsMetropolis,
you could send:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_accept</span></code> to NUTS: nuts={‘target_accept’:0.9}</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transit_p</span></code> to BinaryGibbsMetropolis: binary_gibbs_metropolis={‘transit_p’:.7}</p></li>
</ol>
<p>Note that available names are:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">nuts</span></code>, <code class="docutils literal notranslate"><span class="pre">hmc</span></code>, <code class="docutils literal notranslate"><span class="pre">metropolis</span></code>, <code class="docutils literal notranslate"><span class="pre">binary_metropolis</span></code>,
<code class="docutils literal notranslate"><span class="pre">binary_gibbs_metropolis</span></code>, <code class="docutils literal notranslate"><span class="pre">categorical_gibbs_metropolis</span></code>,
<code class="docutils literal notranslate"><span class="pre">DEMetropolis</span></code>, <code class="docutils literal notranslate"><span class="pre">DEMetropolisZ</span></code>, <code class="docutils literal notranslate"><span class="pre">slice</span></code></p>
</div></blockquote>
</li>
<li><p>If you manually declare the <code class="docutils literal notranslate"><span class="pre">step_method</span></code>s, within the <code class="docutils literal notranslate"><span class="pre">step</span></code>
kwarg, then you can address the <code class="docutils literal notranslate"><span class="pre">step_method</span></code> kwargs directly.
e.g. for a CompoundStep comprising NUTS and BinaryGibbsMetropolis,
you could send</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">step</span><span class="o">=</span><span class="p">[</span><span class="n">pm</span><span class="o">.</span><span class="n">NUTS</span><span class="p">([</span><span class="n">freeRV1</span><span class="p">,</span> <span class="n">freeRV2</span><span class="p">],</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
      <span class="n">pm</span><span class="o">.</span><span class="n">BinaryGibbsMetropolis</span><span class="p">([</span><span class="n">freeRV3</span><span class="p">],</span> <span class="n">transit_p</span><span class="o">=.</span><span class="mi">7</span><span class="p">)]</span>
</pre></div>
</div>
</li>
</ol>
<p>You can find a full list of arguments in the docstring of the step methods.</p>
<p class="rubric">Examples</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="gp">   ...: </span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">   ...: </span><span class="n">h</span> <span class="o">=</span> <span class="mi">61</span>
<span class="gp">   ...: </span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">   ...: </span><span class="n">beta</span> <span class="o">=</span> <span class="mi">2</span>

<span class="gp">In [2]: </span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># context management</span>
<span class="gp">   ...: </span>    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
<span class="gp">   ...: </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
<span class="gp">   ...: </span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="gp">In [3]: </span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;stats&quot;</span><span class="p">)</span>

<span class="gh">Out[3]:</span>
<span class="go">    mean     sd  hdi_3%  hdi_97%</span>
<span class="go">p  0.609  0.047   0.528    0.699</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.sample_posterior_predictive">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">sample_posterior_predictive</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">trace</span></em>, <em class="sig-param"><span class="n">samples</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="model.html#pymc3.model.Model" title="pymc3.model.Model">pymc3.model.Model</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keep_size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>numpy.ndarray<span class="p">]</span><a class="headerlink" href="#pymc3.sampling.sample_posterior_predictive" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate posterior predictive samples from a model given a trace.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>trace</strong><span class="classifier">backend, list, xarray.Dataset, arviz.InferenceData, or MultiTrace</span></dt><dd><p>Trace generated from MCMC sampling, or a list of dicts (eg. points or from find_MAP()),
or xarray.Dataset (eg. InferenceData.posterior or InferenceData.prior)</p>
</dd>
<dt><strong>samples</strong><span class="classifier">int</span></dt><dd><p>Number of posterior predictive samples to generate. Defaults to one posterior predictive
sample per posterior sample, that is, the number of draws times the number of chains. It
is not recommended to modify this value; when modified, some chains may not be represented
in the posterior predictive sample.</p>
</dd>
<dt><strong>model</strong><span class="classifier">Model (optional if in <code class="docutils literal notranslate"><span class="pre">with</span></code> context)</span></dt><dd><p>Model used to generate <code class="docutils literal notranslate"><span class="pre">trace</span></code></p>
</dd>
<dt><strong>vars</strong><span class="classifier">iterable</span></dt><dd><p>Variables for which to compute the posterior predictive samples.
Deprecated: please use <code class="docutils literal notranslate"><span class="pre">var_names</span></code> instead.</p>
</dd>
<dt><strong>var_names</strong><span class="classifier">Iterable[str]</span></dt><dd><p>Names of variables for which to compute the posterior predictive samples.</p>
</dd>
<dt><strong>size</strong><span class="classifier">int</span></dt><dd><p>The number of random draws from the distribution specified by the parameters in each
sample of the trace. Not recommended unless more than ndraws times nchains posterior
predictive samples are needed.</p>
</dd>
<dt><strong>keep_size</strong><span class="classifier">bool, optional</span></dt><dd><p>Force posterior predictive sample to have the same shape as posterior and sample stats
data: <code class="docutils literal notranslate"><span class="pre">(nchains,</span> <span class="pre">ndraws,</span> <span class="pre">...)</span></code>. Overrides samples and size parameters.</p>
</dd>
<dt><strong>random_seed</strong><span class="classifier">int</span></dt><dd><p>Seed for the random number generator.</p>
</dd>
<dt><strong>progressbar</strong><span class="classifier">bool</span></dt><dd><p>Whether or not to display a progress bar in the command line. The bar shows the percentage
of completion, the sampling speed in samples per second (SPS), and the estimated remaining
time until completion (“expected time of arrival”; ETA).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>samples</strong><span class="classifier">dict</span></dt><dd><p>Dictionary with the variable names as keys, and values numpy arrays containing
posterior predictive samples.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.sample_posterior_predictive_w">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">sample_posterior_predictive_w</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traces</span></em>, <em class="sig-param"><span class="n">samples</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">models</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="model.html#pymc3.model.Model" title="pymc3.model.Model">pymc3.model.Model</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>numpy.ndarray<span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.sample_posterior_predictive_w" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate weighted posterior predictive samples from a list of models and
a list of traces according to a set of weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>traces</strong><span class="classifier">list or list of lists</span></dt><dd><p>List of traces generated from MCMC sampling (xarray.Dataset, arviz.InferenceData, or
MultiTrace), or a list of list containing dicts from find_MAP() or points. The number of
traces should be equal to the number of weights.</p>
</dd>
<dt><strong>samples</strong><span class="classifier">int, optional</span></dt><dd><p>Number of posterior predictive samples to generate. Defaults to the
length of the shorter trace in traces.</p>
</dd>
<dt><strong>models</strong><span class="classifier">list of Model</span></dt><dd><p>List of models used to generate the list of traces. The number of models should be equal to
the number of weights and the number of observed RVs should be the same for all models.
By default a single model will be inferred from <code class="docutils literal notranslate"><span class="pre">with</span></code> context, in this case results will
only be meaningful if all models share the same distributions for the observed RVs.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">array-like, optional</span></dt><dd><p>Individual weights for each trace. Default, same weight for each model.</p>
</dd>
<dt><strong>random_seed</strong><span class="classifier">int, optional</span></dt><dd><p>Seed for the random number generator.</p>
</dd>
<dt><strong>progressbar</strong><span class="classifier">bool, optional default True</span></dt><dd><p>Whether or not to display a progress bar in the command line. The bar shows the percentage
of completion, the sampling speed in samples per second (SPS), and the estimated remaining
time until completion (“expected time of arrival”; ETA).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>samples</strong><span class="classifier">dict</span></dt><dd><p>Dictionary with the variables as keys. The values corresponding to the
posterior predictive samples from the weighted models.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="pymc3.sampling.sample_prior_predictive">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">sample_prior_predictive</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">samples</span><span class="o">=</span><span class="default_value">500</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="model.html#pymc3.model.Model" title="pymc3.model.Model">pymc3.model.Model</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Iterable<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>numpy.ndarray<span class="p">]</span><a class="headerlink" href="#pymc3.sampling.sample_prior_predictive" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate samples from the prior predictive distribution.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>samples</strong><span class="classifier">int</span></dt><dd><p>Number of samples from the prior predictive to generate. Defaults to 500.</p>
</dd>
<dt><strong>model</strong><span class="classifier">Model (optional if in <code class="docutils literal notranslate"><span class="pre">with</span></code> context)</span></dt><dd></dd>
<dt><strong>var_names</strong><span class="classifier">Iterable[str]</span></dt><dd><p>A list of names of variables for which to compute the posterior predictive
samples. Defaults to both observed and unobserved RVs.</p>
</dd>
<dt><strong>random_seed</strong><span class="classifier">int</span></dt><dd><p>Seed for the random number generator.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>dict</dt><dd><p>Dictionary with variable names as keys. The values are numpy arrays of prior
samples.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<div class="section" id="step-methods">
<h3>Step-methods<a class="headerlink" href="#step-methods" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="pymc3.sampling.assign_step_methods">
<code class="sig-prename descclassname">pymc3.sampling.</code><code class="sig-name descname">assign_step_methods</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">step=None</em>, <em class="sig-param">methods=(&lt;class 'pymc3.step_methods.hmc.nuts.NUTS'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.hmc.hmc.HamiltonianMC'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.metropolis.Metropolis'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.metropolis.BinaryMetropolis'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.metropolis.BinaryGibbsMetropolis'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.slicer.Slice'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.metropolis.CategoricalGibbsMetropolis'&gt;</em>, <em class="sig-param">&lt;class 'pymc3.step_methods.pgbart.PGBART'&gt;)</em>, <em class="sig-param">step_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.assign_step_methods" title="Permalink to this definition">¶</a></dt>
<dd><p>Assign model variables to appropriate step methods.</p>
<p>Passing a specified model will auto-assign its constituent stochastic
variables to step methods based on the characteristics of the variables.
This function is intended to be called automatically from <code class="docutils literal notranslate"><span class="pre">sample()</span></code>, but
may be called manually. Each step method passed should have a
<code class="docutils literal notranslate"><span class="pre">competence()</span></code> method that returns an ordinal competence value
corresponding to the variable passed to it. This value quantifies the
appropriateness of the step method for sampling the variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model</strong><span class="classifier">Model object</span></dt><dd><p>A fully-specified model object</p>
</dd>
<dt><strong>step</strong><span class="classifier">step function or vector of step functions</span></dt><dd><p>One or more step functions that have been assigned to some subset of
the model’s parameters. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code> (no assigned variables).</p>
</dd>
<dt><strong>methods</strong><span class="classifier">vector of step method classes</span></dt><dd><p>The set of step methods from which the function may choose. Defaults
to the main step methods provided by PyMC3.</p>
</dd>
<dt><strong>step_kwargs</strong><span class="classifier">dict</span></dt><dd><p>Parameters for the samplers. Keys are the lower case names of
the step method, values a dict of arguments.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>methods</strong><span class="classifier">list</span></dt><dd><p>List of step methods associated with the model’s variables.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<div class="section" id="module-pymc3.step_methods.hmc.nuts">
<span id="nuts"></span><h4>NUTS<a class="headerlink" href="#module-pymc3.step_methods.hmc.nuts" title="Permalink to this headline">¶</a></h4>
<dl class="py class">
<dt id="pymc3.step_methods.hmc.nuts.NUTS">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.hmc.nuts.</code><code class="sig-name descname">NUTS</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.nuts.NUTS" title="Permalink to this definition">¶</a></dt>
<dd><p>A sampler for continuous variables based on Hamiltonian mechanics.</p>
<p>NUTS automatically tunes the step size and the number of steps per
sample. A detailed description can be found at [1], “Algorithm 6:
Efficient No-U-Turn Sampler with Dual Averaging”.</p>
<p>NUTS provides a number of statistics that can be accessed with
<cite>trace.get_sampler_stats</cite>:</p>
<ul class="simple">
<li><p><cite>mean_tree_accept</cite>: The mean acceptance probability for the tree
that generated this sample. The mean of these values across all
samples but the burn-in should be approximately <cite>target_accept</cite>
(the default for this is 0.8).</p></li>
<li><p><cite>diverging</cite>: Whether the trajectory for this sample diverged. If
there are any divergences after burnin, this indicates that
the results might not be reliable. Reparametrization can
often help, but you can also try to increase <cite>target_accept</cite> to
something like 0.9 or 0.95.</p></li>
<li><p><cite>energy</cite>: The energy at the point in phase-space where the sample
was accepted. This can be used to identify posteriors with
problematically long tails. See below for an example.</p></li>
<li><p><cite>energy_change</cite>: The difference in energy between the start and
the end of the trajectory. For a perfect integrator this would
always be zero.</p></li>
<li><p><cite>max_energy_change</cite>: The maximum difference in energy along the
whole trajectory.</p></li>
<li><p><cite>depth</cite>: The depth of the tree that was used to generate this sample</p></li>
<li><p><cite>tree_size</cite>: The number of leafs of the sampling tree, when the
sample was accepted. This is usually a bit less than
<cite>2 ** depth</cite>. If the tree size is large, the sampler is
using a lot of leapfrog steps to find the next sample. This can for
example happen if there are strong correlations in the posterior,
if the posterior has long tails, if there are regions of high
curvature (“funnels”), or if the variance estimates in the mass
matrix are inaccurate. Reparametrisation of the model or estimating
the posterior variances from past samples might help.</p></li>
<li><p><cite>tune</cite>: This is <cite>True</cite>, if step size adaptation was turned on when
this sample was generated.</p></li>
<li><p><cite>step_size</cite>: The step size used for this sample.</p></li>
<li><p><cite>step_size_bar</cite>: The current best known step-size. After the tuning
samples, the step size is set to this value. This should converge
during tuning.</p></li>
<li><p><cite>model_logp</cite>: The model log-likelihood for this sample.</p></li>
<li><p><cite>process_time_diff</cite>: The time it took to draw the sample, as defined
by the python standard library <cite>time.process_time</cite>. This counts all
the CPU time, including worker processes in BLAS and OpenMP.</p></li>
<li><p><cite>perf_counter_diff</cite>: The time it took to draw the sample, as defined
by the python standard library <cite>time.perf_counter</cite> (wall time).</p></li>
<li><p><cite>perf_counter_start</cite>: The value of <cite>time.perf_counter</cite> at the beginning
of the computation of the draw.</p></li>
</ul>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="rb9da66cafc36-1"><span class="brackets">1</span></dt>
<dd><p>Hoffman, Matthew D., &amp; Gelman, Andrew. (2011). The No-U-Turn
Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.</p>
</dd>
</dl>
<p>Set up the No-U-Turn sampler.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list of Theano variables, default all continuous vars</strong></dt><dd></dd>
<dt><strong>Emax: float, default 1000</strong></dt><dd><p>Maximum energy change allowed during leapfrog steps. Larger
deviations will abort the integration.</p>
</dd>
<dt><strong>target_accept: float, default .8</strong></dt><dd><p>Adapt the step size such that the average acceptance
probability across the trajectories are close to target_accept.
Higher values for target_accept lead to smaller step sizes.
Setting this to higher values like 0.9 or 0.99 can help
with sampling from difficult posteriors. Valid values are
between 0 and 1 (exclusive).</p>
</dd>
<dt><strong>step_scale: float, default 0.25</strong></dt><dd><p>Size of steps to take, automatically scaled down by <cite>1/n**(1/4)</cite>.
If step size adaptation is switched off, the resulting step size
is used. If adaptation is enabled, it is used as initial guess.</p>
</dd>
<dt><strong>gamma: float, default .05</strong></dt><dd></dd>
<dt><strong>k: float, default .75</strong></dt><dd><p>Parameter for dual averaging for step size adaptation. Values
between 0.5 and 1 (exclusive) are admissible. Higher values
correspond to slower adaptation.</p>
</dd>
<dt><strong>t0: int, default 10</strong></dt><dd><p>Parameter for dual averaging. Higher values slow initial
adaptation.</p>
</dd>
<dt><strong>adapt_step_size: bool, default=True</strong></dt><dd><p>Whether step size adaptation should be enabled. If this is
disabled, <cite>k</cite>, <cite>t0</cite>, <cite>gamma</cite> and <cite>target_accept</cite> are ignored.</p>
</dd>
<dt><strong>max_treedepth: int, default=10</strong></dt><dd><p>The maximum tree depth. Trajectories are stopped when this
depth is reached.</p>
</dd>
<dt><strong>early_max_treedepth: int, default=8</strong></dt><dd><p>The maximum tree depth during the first 200 tuning samples.</p>
</dd>
<dt><strong>scaling: array_like, ndim = {1,2}</strong></dt><dd><p>The inverse mass, or precision matrix. One dimensional arrays are
interpreted as diagonal matrices. If <cite>is_cov</cite> is set to True,
this will be interpreded as the mass or covariance matrix.</p>
</dd>
<dt><strong>is_cov: bool, default=False</strong></dt><dd><p>Treat the scaling as mass or covariance matrix.</p>
</dd>
<dt><strong>potential: Potential, optional</strong></dt><dd><p>An object that represents the Hamiltonian with methods <cite>velocity</cite>,
<cite>energy</cite>, and <cite>random</cite> methods. It can be specified instead
of the scaling matrix.</p>
</dd>
<dt><strong>model: pymc3.Model</strong></dt><dd><p>The model</p>
</dd>
<dt><strong>kwargs: passed to BaseHMC</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The step size adaptation stops when <cite>self.tune</cite> is set to False.
This is usually achieved by setting the <cite>tune</cite> parameter if
<cite>pm.sample</cite> to the desired number of tuning steps.</p>
<dl class="py method">
<dt id="pymc3.step_methods.hmc.nuts.NUTS.competence">
<em class="property">static </em><code class="sig-name descname">competence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">var</span></em>, <em class="sig-param"><span class="n">has_grad</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.nuts.NUTS.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>Check how appropriate this class is for sampling a random variable.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pymc3.step_methods.metropolis">
<span id="metropolis"></span><h4>Metropolis<a class="headerlink" href="#module-pymc3.step_methods.metropolis" title="Permalink to this headline">¶</a></h4>
<dl class="py class">
<dt id="pymc3.step_methods.metropolis.BinaryGibbsMetropolis">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">BinaryGibbsMetropolis</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryGibbsMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>A Metropolis-within-Gibbs step method optimized for binary variables</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler</p>
</dd>
<dt><strong>order: list or ‘random’</strong></dt><dd><p>List of integers indicating the Gibbs update order
e.g., [0, 2, 1, …]. Default is random</p>
</dd>
<dt><strong>transit_p: float</strong></dt><dd><p>The diagonal of the transition kernel. A value &gt; .5 gives anticorrelated proposals,
which resulting in more efficient antithetical sampling. Default is 0.8</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.step_methods.metropolis.BinaryGibbsMetropolis.competence">
<em class="property">static </em><code class="sig-name descname">competence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">var</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryGibbsMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>BinaryMetropolis is only suitable for Bernoulli
and Categorical variables with k=2.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.step_methods.metropolis.BinaryMetropolis">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">BinaryMetropolis</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Metropolis-Hastings optimized for binary variables</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler</p>
</dd>
<dt><strong>scaling: scalar or array</strong></dt><dd><p>Initial scale factor for proposal. Defaults to 1.</p>
</dd>
<dt><strong>tune: bool</strong></dt><dd><p>Flag for tuning. Defaults to True.</p>
</dd>
<dt><strong>tune_interval: int</strong></dt><dd><p>The frequency of tuning. Defaults to 100 iterations.</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.step_methods.metropolis.BinaryMetropolis.competence">
<em class="property">static </em><code class="sig-name descname">competence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">var</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>BinaryMetropolis is only suitable for binary (bool)
and Categorical variables with k=1.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.step_methods.metropolis.CategoricalGibbsMetropolis">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">CategoricalGibbsMetropolis</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.CategoricalGibbsMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>A Metropolis-within-Gibbs step method optimized for categorical variables.
This step method works for Bernoulli variables as well, but it is not
optimized for them, like BinaryGibbsMetropolis is. Step method supports
two types of proposals: A uniform proposal and a proportional proposal,
which was introduced by Liu in his 1996 technical report
“Metropolized Gibbs Sampler: An Improvement”.</p>
<dl class="py method">
<dt id="pymc3.step_methods.metropolis.CategoricalGibbsMetropolis.competence">
<em class="property">static </em><code class="sig-name descname">competence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">var</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.CategoricalGibbsMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>CategoricalGibbsMetropolis is only suitable for Bernoulli and
Categorical variables.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.step_methods.metropolis.DEMetropolis">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">DEMetropolis</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.DEMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Differential Evolution Metropolis sampling step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>lamb: float</strong></dt><dd><p>Lambda parameter of the DE proposal mechanism. Defaults to 2.38 / sqrt(2 * ndim)</p>
</dd>
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler</p>
</dd>
<dt><strong>S: standard deviation or covariance matrix</strong></dt><dd><p>Some measure of variance to parameterize proposal distribution</p>
</dd>
<dt><strong>proposal_dist: function</strong></dt><dd><p>Function that returns zero-mean deviates when parameterized with
S (and n). Defaults to Uniform(-S,+S).</p>
</dd>
<dt><strong>scaling: scalar or array</strong></dt><dd><p>Initial scale factor for epsilon. Defaults to 0.001</p>
</dd>
<dt><strong>tune: str</strong></dt><dd><p>Which hyperparameter to tune. Defaults to None, but can also be ‘scaling’ or ‘lambda’.</p>
</dd>
<dt><strong>tune_interval: int</strong></dt><dd><p>The frequency of tuning. Defaults to 100 iterations.</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
<dt><strong>mode:  string or `Mode` instance.</strong></dt><dd><p>compilation mode passed to Theano functions</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r702cfac9e2a0-braak2006"><span class="brackets">Braak2006</span></dt>
<dd><p>Cajo C.F. ter Braak (2006).
A Markov Chain Monte Carlo version of the genetic algorithm
Differential Evolution: easy Bayesian computing for real parameter spaces.
Statistics and Computing
<a class="reference external" href="https://doi.org/10.1007/s11222-006-8769-1">link</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list of sampling variables</strong></dt><dd></dd>
<dt><strong>shared: dict of theano variable -&gt; shared variable</strong></dt><dd></dd>
<dt><strong>blocked: Boolean (default True)</strong></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="pymc3.step_methods.metropolis.DEMetropolisZ">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">DEMetropolisZ</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.DEMetropolisZ" title="Permalink to this definition">¶</a></dt>
<dd><p>Adaptive Differential Evolution Metropolis sampling step that uses the past to inform jumps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>lamb: float</strong></dt><dd><p>Lambda parameter of the DE proposal mechanism. Defaults to 2.38 / sqrt(2 * ndim)</p>
</dd>
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler</p>
</dd>
<dt><strong>S: standard deviation or covariance matrix</strong></dt><dd><p>Some measure of variance to parameterize proposal distribution</p>
</dd>
<dt><strong>proposal_dist: function</strong></dt><dd><p>Function that returns zero-mean deviates when parameterized with
S (and n). Defaults to Uniform(-S,+S).</p>
</dd>
<dt><strong>scaling: scalar or array</strong></dt><dd><p>Initial scale factor for epsilon. Defaults to 0.001</p>
</dd>
<dt><strong>tune: str</strong></dt><dd><p>Which hyperparameter to tune. Defaults to ‘lambda’, but can also be ‘scaling’ or None.</p>
</dd>
<dt><strong>tune_interval: int</strong></dt><dd><p>The frequency of tuning. Defaults to 100 iterations.</p>
</dd>
<dt><strong>tune_drop_fraction: float</strong></dt><dd><p>Fraction of tuning steps that will be removed from the samplers history when the tuning ends.
Defaults to 0.9 - keeping the last 10% of tuning steps for good mixing while removing 90% of
potentially unconverged tuning positions.</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
<dt><strong>mode:  string or `Mode` instance.</strong></dt><dd><p>compilation mode passed to Theano functions</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r5747893c210c-braak2006"><span class="brackets">Braak2006</span></dt>
<dd><p>Cajo C.F. ter Braak (2006).
Differential Evolution Markov Chain with snooker updater and fewer chains.
Statistics and Computing
<a class="reference external" href="https://doi.org/10.1007/s11222-008-9104-9">link</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list of sampling variables</strong></dt><dd></dd>
<dt><strong>shared: dict of theano variable -&gt; shared variable</strong></dt><dd></dd>
<dt><strong>blocked: Boolean (default True)</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.step_methods.metropolis.DEMetropolisZ.reset_tuning">
<code class="sig-name descname">reset_tuning</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.DEMetropolisZ.reset_tuning" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the tuned sampler parameters and history to their initial values.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.step_methods.metropolis.DEMetropolisZ.stop_tuning">
<code class="sig-name descname">stop_tuning</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.DEMetropolisZ.stop_tuning" title="Permalink to this definition">¶</a></dt>
<dd><p>At the end of the tuning phase, this method removes the first x% of the history
so future proposals are not informed by unconverged tuning iterations.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.step_methods.metropolis.Metropolis">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.metropolis.</code><code class="sig-name descname">Metropolis</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.Metropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Metropolis-Hastings sampling step</p>
<p>Create an instance of a Metropolis stepper</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler</p>
</dd>
<dt><strong>S: standard deviation or covariance matrix</strong></dt><dd><p>Some measure of variance to parameterize proposal distribution</p>
</dd>
<dt><strong>proposal_dist: function</strong></dt><dd><p>Function that returns zero-mean deviates when parameterized with
S (and n). Defaults to normal.</p>
</dd>
<dt><strong>scaling: scalar or array</strong></dt><dd><p>Initial scale factor for proposal. Defaults to 1.</p>
</dd>
<dt><strong>tune: bool</strong></dt><dd><p>Flag for tuning. Defaults to True.</p>
</dd>
<dt><strong>tune_interval: int</strong></dt><dd><p>The frequency of tuning. Defaults to 100 iterations.</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
<dt><strong>mode: string or `Mode` instance.</strong></dt><dd><p>compilation mode passed to Theano functions</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.step_methods.metropolis.Metropolis.reset_tuning">
<code class="sig-name descname">reset_tuning</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.Metropolis.reset_tuning" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the tuned sampler parameters to their initial values.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pymc3.step_methods.slicer">
<span id="slice"></span><h4>Slice<a class="headerlink" href="#module-pymc3.step_methods.slicer" title="Permalink to this headline">¶</a></h4>
<dl class="py class">
<dt id="pymc3.step_methods.slicer.Slice">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.slicer.</code><code class="sig-name descname">Slice</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.slicer.Slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Univariate slice sampler step method</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list</strong></dt><dd><p>List of variables for sampler.</p>
</dd>
<dt><strong>w: float</strong></dt><dd><p>Initial width of slice (Defaults to 1).</p>
</dd>
<dt><strong>tune: bool</strong></dt><dd><p>Flag for tuning (Defaults to True).</p>
</dd>
<dt><strong>model: PyMC Model</strong></dt><dd><p>Optional model for sampling step. Defaults to None (taken from context).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="hamiltonian-monte-carlo">
<h4>Hamiltonian Monte Carlo<a class="headerlink" href="#hamiltonian-monte-carlo" title="Permalink to this headline">¶</a></h4>
<dl class="py class">
<dt id="pymc3.step_methods.hmc.hmc.HamiltonianMC">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.step_methods.hmc.hmc.</code><code class="sig-name descname">HamiltonianMC</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.hmc.HamiltonianMC" title="Permalink to this definition">¶</a></dt>
<dd><p>A sampler for continuous variables based on Hamiltonian mechanics.</p>
<p>See NUTS sampler for automatically tuned stopping time and step size scaling.</p>
<p>Set up the Hamiltonian Monte Carlo sampler.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vars: list of theano variables</strong></dt><dd></dd>
<dt><strong>path_length: float, default=2</strong></dt><dd><p>total length to travel</p>
</dd>
<dt><strong>step_rand: function float -&gt; float, default=unif</strong></dt><dd><p>A function which takes the step size and returns an new one used to
randomize the step size at each iteration.</p>
</dd>
<dt><strong>step_scale: float, default=0.25</strong></dt><dd><p>Initial size of steps to take, automatically scaled down
by 1/n**(1/4).</p>
</dd>
<dt><strong>scaling: array_like, ndim = {1,2}</strong></dt><dd><p>The inverse mass, or precision matrix. One dimensional arrays are
interpreted as diagonal matrices. If <cite>is_cov</cite> is set to True,
this will be interpreded as the mass or covariance matrix.</p>
</dd>
<dt><strong>is_cov: bool, default=False</strong></dt><dd><p>Treat the scaling as mass or covariance matrix.</p>
</dd>
<dt><strong>potential: Potential, optional</strong></dt><dd><p>An object that represents the Hamiltonian with methods <cite>velocity</cite>,
<cite>energy</cite>, and <cite>random</cite> methods. It can be specified instead
of the scaling matrix.</p>
</dd>
<dt><strong>target_accept: float, default 0.65</strong></dt><dd><p>Adapt the step size such that the average acceptance
probability across the trajectories are close to target_accept.
Higher values for target_accept lead to smaller step sizes.
Setting this to higher values like 0.9 or 0.99 can help
with sampling from difficult posteriors. Valid values are
between 0 and 1 (exclusive). Default of 0.65 is from (Beskos et.
al. 2010, Neal 2011). See Hoffman and Gelman’s “The No-U-Turn
Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte
Carlo” section 3.2 for details.</p>
</dd>
<dt><strong>gamma: float, default .05</strong></dt><dd></dd>
<dt><strong>k: float, default .75</strong></dt><dd><p>Parameter for dual averaging for step size adaptation. Values
between 0.5 and 1 (exclusive) are admissible. Higher values
correspond to slower adaptation.</p>
</dd>
<dt><strong>t0: float &gt; 0, default 10</strong></dt><dd><p>Parameter for dual averaging. Higher values slow initial
adaptation.</p>
</dd>
<dt><strong>adapt_step_size: bool, default=True</strong></dt><dd><p>Whether step size adaptation should be enabled. If this is
disabled, <cite>k</cite>, <cite>t0</cite>, <cite>gamma</cite> and <cite>target_accept</cite> are ignored.</p>
</dd>
<dt><strong>max_steps: int</strong></dt><dd><p>The maximum number of leapfrog steps.</p>
</dd>
<dt><strong>model: pymc3.Model</strong></dt><dd><p>The model</p>
</dd>
<dt><strong>**kwargs: passed to BaseHMC</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.step_methods.hmc.hmc.HamiltonianMC.competence">
<em class="property">static </em><code class="sig-name descname">competence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">var</span></em>, <em class="sig-param"><span class="n">has_grad</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.hmc.HamiltonianMC.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>Check how appropriate this class is for sampling a random variable.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential-monte-carlo">
<h4>Sequential Monte Carlo<a class="headerlink" href="#sequential-monte-carlo" title="Permalink to this headline">¶</a></h4>
<dl class="py class">
<dt id="pymc3.smc.smc.SMC">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.smc.smc.</code><code class="sig-name descname">SMC</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">draws</span><span class="o">=</span><span class="default_value">2000</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="o">=</span><span class="default_value">'metropolis'</span></em>, <em class="sig-param"><span class="n">n_steps</span><span class="o">=</span><span class="default_value">25</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tune_steps</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">p_acc_rate</span><span class="o">=</span><span class="default_value">0.85</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">save_sim_data</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">save_log_pseudolikelihood</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">chain</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequential Monte Carlo with Independent Metropolis-Hastings and ABC kernels.</p>
<dl class="py method">
<dt id="pymc3.smc.smc.SMC.initialize_logp">
<code class="sig-name descname">initialize_logp</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.initialize_logp" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the prior and likelihood log probabilities.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.initialize_population">
<code class="sig-name descname">initialize_population</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.initialize_population" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an initial population from the prior distribution.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.mutate">
<code class="sig-name descname">mutate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.mutate" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent Metropolis-Hastings perturbation.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.posterior_to_trace">
<code class="sig-name descname">posterior_to_trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.posterior_to_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Save results into a PyMC3 trace.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.resample">
<code class="sig-name descname">resample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.resample" title="Permalink to this definition">¶</a></dt>
<dd><p>Resample particles based on importance weights.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.setup_kernel">
<code class="sig-name descname">setup_kernel</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.setup_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set up the likelihood logp function based on the chosen kernel.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.tune">
<code class="sig-name descname">tune</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Tune n_steps based on the acceptance rate.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.update_proposal">
<code class="sig-name descname">update_proposal</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.update_proposal" title="Permalink to this definition">¶</a></dt>
<dd><p>Update proposal based on the covariance matrix from tempered posterior.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.smc.smc.SMC.update_weights_beta">
<code class="sig-name descname">update_weights_beta</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.smc.smc.SMC.update_weights_beta" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the next inverse temperature (beta).</p>
<p>The importance weights based on current beta and tempered likelihood and updates the
marginal likelihood estimate.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="multitrace">
<h3>MultiTrace<a class="headerlink" href="#multitrace" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="pymc3.backends.base.MultiTrace">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.backends.base.</code><code class="sig-name descname">MultiTrace</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">straces</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace" title="Permalink to this definition">¶</a></dt>
<dd><p>Main interface for accessing values from MCMC results.</p>
<p>The core method to select values is <cite>get_values</cite>. The method
to select sampler statistics is <cite>get_sampler_stats</cite>. Both kinds of
values can also be accessed by indexing the MultiTrace object.
Indexing can behave in four ways:</p>
<ol class="arabic">
<li><p>Indexing with a variable or variable name (str) returns all
values for that variable, combining values for all chains.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trace</span><span class="p">[</span><span class="n">varname</span><span class="p">]</span>
</pre></div>
</div>
<p>Slicing after the variable name can be used to burn and thin
the samples.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trace</span><span class="p">[</span><span class="n">varname</span><span class="p">,</span> <span class="mi">1000</span><span class="p">:]</span>
</pre></div>
</div>
<p>For convenience during interactive use, values can also be
accessed using the variable as an attribute.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trace</span><span class="o">.</span><span class="n">varname</span>
</pre></div>
</div>
</li>
<li><p>Indexing with an integer returns a dictionary with values for
each variable at the given index (corresponding to a single
sampling iteration).</p></li>
<li><p>Slicing with a range returns a new trace with the number of draws
corresponding to the range.</p></li>
<li><p>Indexing with the name of a sampler statistic that is not also
the name of a variable returns those values from all chains.
If there is more than one sampler that provides that statistic,
the values are concatenated along a new axis.</p></li>
</ol>
<p>For any methods that require a single trace (e.g., taking the length
of the MultiTrace instance, which returns the number of draws), the
trace with the highest chain number is always used.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>nchains: int</strong></dt><dd><p>Number of chains in the <cite>MultiTrace</cite>.</p>
</dd>
<dt><strong>chains: `List[int]`</strong></dt><dd><p>List of chain indices</p>
</dd>
<dt><strong>report: str</strong></dt><dd><p>Report on the sampling process.</p>
</dd>
<dt><strong>varnames: `List[str]`</strong></dt><dd><p>List of variable names in the trace(s)</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.add_values">
<code class="sig-name descname">add_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vals</span></em>, <em class="sig-param"><span class="n">overwrite</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#pymc3.backends.base.MultiTrace.add_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Add variables to traces.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>vals: dict (str: array-like)</strong></dt><dd><p>The keys should be the names of the new variables. The values are expected to be
array-like objects. For traces with more than one chain the length of each value
should match the number of total samples already in the trace <cite>(chains * iterations)</cite>,
otherwise a warning is raised.</p>
</dd>
<dt><strong>overwrite: bool</strong></dt><dd><p>If <cite>False</cite> (default) a ValueError is raised if the variable already exists.
Change to <cite>True</cite> to overwrite the values of variables</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>None.</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.get_sampler_stats">
<code class="sig-name descname">get_sampler_stats</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stat_name</span></em>, <em class="sig-param"><span class="n">burn</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">thin</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">combine</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">chains</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">squeeze</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace.get_sampler_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get sampler statistics from the trace.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>stat_name: str</strong></dt><dd></dd>
<dt><strong>sampler_idx: int or None</strong></dt><dd></dd>
<dt><strong>burn: int</strong></dt><dd></dd>
<dt><strong>thin: int</strong></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>If the <cite>sampler_idx</cite> is specified, return the statistic with</dt><dd></dd>
<dt>the given name in a numpy array. If it is not specified and there</dt><dd></dd>
<dt>is more than one sampler that provides this statistic, return</dt><dd></dd>
<dt>a numpy array of shape (m, n), where <cite>m</cite> is the number of</dt><dd></dd>
<dt>such samplers, and <cite>n</cite> is the number of samples.</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.get_values">
<code class="sig-name descname">get_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">varname</span></em>, <em class="sig-param"><span class="n">burn</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">thin</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">combine</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">chains</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">squeeze</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace.get_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Get values from traces.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>varname: str</strong></dt><dd></dd>
<dt><strong>burn: int</strong></dt><dd></dd>
<dt><strong>thin: int</strong></dt><dd></dd>
<dt><strong>combine: bool</strong></dt><dd><p>If True, results from <cite>chains</cite> will be concatenated.</p>
</dd>
<dt><strong>chains: int or list of ints</strong></dt><dd><p>Chains to retrieve. If None, all chains are used. A single
chain value can also be given.</p>
</dd>
<dt><strong>squeeze: bool</strong></dt><dd><p>Return a single array element if the resulting list of
values only has one element. If False, the result will
always be a list of arrays, even if <cite>combine</cite> is True.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>A list of NumPy arrays or a single NumPy array (depending on</dt><dd></dd>
<dt><cite>squeeze</cite>).</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.point">
<code class="sig-name descname">point</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">idx</span></em>, <em class="sig-param"><span class="n">chain</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace.point" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary of point values at <cite>idx</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>idx: int</strong></dt><dd></dd>
<dt><strong>chain: int</strong></dt><dd><p>If a chain is not given, the highest chain number is used.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.points">
<code class="sig-name descname">points</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">chains</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace.points" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterator over all or some of the sample points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>chains: list of int or N</strong></dt><dd><p>The chains whose points should be inlcuded in the iterator.  If
chains is not given, include points from all chains.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.backends.base.MultiTrace.remove_values">
<code class="sig-name descname">remove_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.MultiTrace.remove_values" title="Permalink to this definition">¶</a></dt>
<dd><p>remove variables from traces.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>name: str</strong></dt><dd><p>Name of the variable to remove. Raises KeyError if the variable is not present</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.backends.base.BaseTrace">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.backends.base.</code><code class="sig-name descname">BaseTrace</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">vars</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">test_point</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.backends.base.BaseTrace" title="Permalink to this definition">¶</a></dt>
<dd><p>Base trace object</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>name: str</strong></dt><dd><p>Name of backend</p>
</dd>
<dt><strong>model: Model</strong></dt><dd><p>If None, the model is taken from the <cite>with</cite> context.</p>
</dd>
<dt><strong>vars: list of variables</strong></dt><dd><p>Sampling values will be stored for these variables. If None,
<cite>model.unobserved_RVs</cite> is used.</p>
</dd>
<dt><strong>test_point: dict</strong></dt><dd><p>use different test point that might be with changed variables shapes</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="variational-inference">
<h2>Variational Inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-pymc3.variational.opvi">
<span id="opvi"></span><h3>OPVI<a class="headerlink" href="#module-pymc3.variational.opvi" title="Permalink to this headline">¶</a></h3>
<p>Variational inference is a great approach for doing really complex,
often intractable Bayesian inference in approximate form. Common methods
(e.g. ADVI) lack from complexity so that approximate posterior does not
reveal the true nature of underlying problem. In some applications it can
yield unreliable decisions.</p>
<p>Recently on NIPS 2017 <a class="reference external" href="https://arxiv.org/abs/1610.09033/">OPVI</a> framework
was presented. It generalizes variational inference so that the problem is
build with blocks. The first and essential block is Model itself. Second is
Approximation, in some cases <span class="math notranslate nohighlight">\(log Q(D)\)</span> is not really needed. Necessity
depends on the third and fourth part of that black box, Operator and
Test Function respectively.</p>
<p>Operator is like an approach we use, it constructs loss from given Model,
Approximation and Test Function. The last one is not needed if we minimize
KL Divergence from Q to posterior. As a drawback we need to compute <span class="math notranslate nohighlight">\(loq Q(D)\)</span>.
Sometimes approximation family is intractable and <span class="math notranslate nohighlight">\(loq Q(D)\)</span> is not available,
here comes LS(Langevin Stein) Operator with a set of test functions.</p>
<p>Test Function has more unintuitive meaning. It is usually used with LS operator
and represents all we want from our approximate distribution. For any given vector
based function of <span class="math notranslate nohighlight">\(z\)</span> LS operator yields zero mean function under posterior.
<span class="math notranslate nohighlight">\(loq Q(D)\)</span> is no more needed. That opens a door to rich approximation
families as neural networks.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei
Operator Variational Inference
<a class="reference external" href="https://arxiv.org/abs/1610.09033">https://arxiv.org/abs/1610.09033</a> (2016)</p></li>
</ul>
<dl class="py class">
<dt id="pymc3.variational.opvi.Approximation">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.opvi.</code><code class="sig-name descname">Approximation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">groups</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Wrapper for grouped approximations</strong></p>
<p>Wraps list of groups, creates an Approximation instance that collects
sampled variables from all the groups, also collects logQ needed for
explicit Variational Inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>groups: list[Group]</strong></dt><dd><p>List of <a class="reference internal" href="#pymc3.variational.opvi.Group" title="pymc3.variational.opvi.Group"><code class="xref py py-class docutils literal notranslate"><span class="pre">Group</span></code></a> instances. They should have all model variables</p>
</dd>
<dt><strong>model: Model</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#pymc3.variational.opvi.Group" title="pymc3.variational.opvi.Group"><code class="xref py py-class docutils literal notranslate"><span class="pre">Group</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>Some shortcuts for single group approximations are available:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MeanField</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">FullRank</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">NormalizingFlow</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Empirical</span></code></p></li>
</ul>
</div></blockquote>
<p>Single group accepts <cite>local_rv</cite> keyword with dict mapping PyMC3 variables
to their local Group parameters dict</p>
<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.datalogp">
<em class="property">property </em><code class="sig-name descname">datalogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.datalogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes <span class="math notranslate nohighlight">\(E_{q}(data term)\)</span> from model via <cite>theano.scan</cite> that can be optimized later</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.datalogp_norm">
<em class="property">property </em><code class="sig-name descname">datalogp_norm</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.datalogp_norm" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - normalized <span class="math notranslate nohighlight">\(E_{q}(data term)\)</span></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.get_optimization_replacements">
<code class="sig-name descname">get_optimization_replacements</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.get_optimization_replacements" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - optimizations for logP. If sample size is static and equal to 1:
then <cite>theano.scan</cite> MC estimate is replaced with single sample without call to <cite>theano.scan</cite>.</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.logp">
<em class="property">property </em><code class="sig-name descname">logp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.logp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes <span class="math notranslate nohighlight">\(E_{q}(logP)\)</span> from model via <cite>theano.scan</cite> that can be optimized later</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.logp_norm">
<em class="property">property </em><code class="sig-name descname">logp_norm</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.logp_norm" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - normalized <span class="math notranslate nohighlight">\(E_{q}(logP)\)</span></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.logq">
<em class="property">property </em><code class="sig-name descname">logq</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.logq" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - collects <cite>logQ</cite> for all groups</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.logq_norm">
<em class="property">property </em><code class="sig-name descname">logq_norm</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.logq_norm" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - collects <cite>logQ</cite> for all groups and normalizes it</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.make_size_and_deterministic_replacements">
<code class="sig-name descname">make_size_and_deterministic_replacements</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">d</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.make_size_and_deterministic_replacements" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - creates correct replacements for initial depending on
sample size and deterministic flag</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>s: scalar</strong></dt><dd><p>sample size</p>
</dd>
<dt><strong>d: bool</strong></dt><dd><p>whether sampling is done deterministically</p>
</dd>
<dt><strong>more_replacements: dict</strong></dt><dd><p>replacements for shape and initial</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>dict with replacements for initial</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.replacements">
<em class="property">property </em><code class="sig-name descname">replacements</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.replacements" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - all replacements from groups to replace PyMC random variables with approximation</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.rslice">
<code class="sig-name descname">rslice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.rslice" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - vectorized sampling for named random variable without call to <cite>theano.scan</cite>.
This node still needs <a class="reference internal" href="#pymc3.variational.opvi.Approximation.set_size_and_deterministic" title="pymc3.variational.opvi.Approximation.set_size_and_deterministic"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_size_and_deterministic()</span></code></a> to be evaluated</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">draws</span><span class="o">=</span><span class="default_value">500</span></em>, <em class="sig-param"><span class="n">include_transformed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from variational posterior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>draws: `int`</strong></dt><dd><p>Number of random samples.</p>
</dd>
<dt><strong>include_transformed: `bool`</strong></dt><dd><p>If True, transformed variables are also sampled. Default is False.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>trace: <a class="reference internal" href="#pymc3.backends.base.MultiTrace" title="pymc3.backends.base.MultiTrace"><code class="xref py py-class docutils literal notranslate"><span class="pre">pymc3.backends.base.MultiTrace</span></code></a></dt><dd><p>Samples drawn from variational posterior.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.sample_node">
<code class="sig-name descname">sample_node</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em>, <em class="sig-param"><span class="n">size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">deterministic</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sample_node" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples given node or nodes over shared posterior</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>node: Theano Variables (or Theano expressions)</strong></dt><dd></dd>
<dt><strong>size: None or scalar</strong></dt><dd><p>number of samples</p>
</dd>
<dt><strong>more_replacements: `dict`</strong></dt><dd><p>add custom replacements to graph, e.g. change input source</p>
</dd>
<dt><strong>deterministic: bool</strong></dt><dd><p>whether to use zeros as initial distribution
if True - zero initial point will produce constant latent variables</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>sampled node(s) with replacements</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.scale_cost_to_minibatch">
<em class="property">property </em><code class="sig-name descname">scale_cost_to_minibatch</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.scale_cost_to_minibatch" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - Property to control scaling cost to minibatch</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.set_size_and_deterministic">
<code class="sig-name descname">set_size_and_deterministic</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em>, <em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">d</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.set_size_and_deterministic" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - after node is sampled via <a class="reference internal" href="#pymc3.variational.opvi.Approximation.symbolic_sample_over_posterior" title="pymc3.variational.opvi.Approximation.symbolic_sample_over_posterior"><code class="xref py py-func docutils literal notranslate"><span class="pre">symbolic_sample_over_posterior()</span></code></a> or
<a class="reference internal" href="#pymc3.variational.opvi.Approximation.symbolic_single_sample" title="pymc3.variational.opvi.Approximation.symbolic_single_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">symbolic_single_sample()</span></code></a> new random generator can be allocated and applied to node</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>node: :class:`Variable`</strong></dt><dd><p>Theano node with symbolically applied VI replacements</p>
</dd>
<dt><strong>s: scalar</strong></dt><dd><p>desired number of samples</p>
</dd>
<dt><strong>d: bool or int</strong></dt><dd><p>whether sampling is done deterministically</p>
</dd>
<dt><strong>more_replacements: dict</strong></dt><dd><p>more replacements to apply</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> with applied replacements, ready to use</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.single_symbolic_datalogp">
<em class="property">property </em><code class="sig-name descname">single_symbolic_datalogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.single_symbolic_datalogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - for single MC sample estimate of <span class="math notranslate nohighlight">\(E_{q}(data term)\)</span> <cite>theano.scan</cite>
is not needed and code can be optimized</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.single_symbolic_logp">
<em class="property">property </em><code class="sig-name descname">single_symbolic_logp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.single_symbolic_logp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - for single MC sample estimate of <span class="math notranslate nohighlight">\(E_{q}(logP)\)</span> <cite>theano.scan</cite>
is not needed and code can be optimized</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.single_symbolic_varlogp">
<em class="property">property </em><code class="sig-name descname">single_symbolic_varlogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.single_symbolic_varlogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - for single MC sample estimate of <span class="math notranslate nohighlight">\(E_{q}(prior term)\)</span> <cite>theano.scan</cite>
is not needed and code can be optimized</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.sized_symbolic_datalogp">
<em class="property">property </em><code class="sig-name descname">sized_symbolic_datalogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sized_symbolic_datalogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes sampled data term from model via <cite>theano.scan</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.sized_symbolic_logp">
<em class="property">property </em><code class="sig-name descname">sized_symbolic_logp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sized_symbolic_logp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes sampled logP from model via <cite>theano.scan</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.sized_symbolic_varlogp">
<em class="property">property </em><code class="sig-name descname">sized_symbolic_varlogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sized_symbolic_varlogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes sampled prior term from model via <cite>theano.scan</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.symbolic_logq">
<em class="property">property </em><code class="sig-name descname">symbolic_logq</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.symbolic_logq" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - collects <cite>symbolic_logq</cite> for all groups</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.symbolic_normalizing_constant">
<em class="property">property </em><code class="sig-name descname">symbolic_normalizing_constant</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.symbolic_normalizing_constant" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - normalizing constant for <cite>self.logq</cite>, scales it to <cite>minibatch_size</cite> instead of <cite>total_size</cite>.
Here the effect is controlled by <cite>self.scale_cost_to_minibatch</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.symbolic_sample_over_posterior">
<code class="sig-name descname">symbolic_sample_over_posterior</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.symbolic_sample_over_posterior" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - performs sampling of node applying independent samples from posterior each time.
Note that it is done symbolically and this node needs <a class="reference internal" href="#pymc3.variational.opvi.Approximation.set_size_and_deterministic" title="pymc3.variational.opvi.Approximation.set_size_and_deterministic"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_size_and_deterministic()</span></code></a> call</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.symbolic_single_sample">
<code class="sig-name descname">symbolic_single_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.symbolic_single_sample" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - performs sampling of node applying single sample from posterior.
Note that it is done symbolically and this node needs
<a class="reference internal" href="#pymc3.variational.opvi.Approximation.set_size_and_deterministic" title="pymc3.variational.opvi.Approximation.set_size_and_deterministic"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_size_and_deterministic()</span></code></a> call with <cite>size=1</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.to_flat_input">
<code class="sig-name descname">to_flat_input</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.to_flat_input" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - replace vars with flattened view stored in <cite>self.inputs</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.varlogp">
<em class="property">property </em><code class="sig-name descname">varlogp</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.varlogp" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - computes <span class="math notranslate nohighlight">\(E_{q}(prior term)\)</span> from model via <cite>theano.scan</cite> that can be optimized later</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Approximation.varlogp_norm">
<em class="property">property </em><code class="sig-name descname">varlogp_norm</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.varlogp_norm" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - normalized <span class="math notranslate nohighlight">\(E_{q}(prior term)\)</span></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.opvi.Group">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.opvi.</code><code class="sig-name descname">Group</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">group</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">vfam</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">params</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Base class for grouping variables in VI</strong></p>
<p>Grouped Approximation is used for modelling mutual dependencies
for a specified group of variables. Base for local and global group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>group: list</strong></dt><dd><p>List of PyMC3 variables or None indicating that group takes all the rest variables</p>
</dd>
<dt><strong>vfam: str</strong></dt><dd><p>String that marks the corresponding variational family for the group.
Cannot be passed both with <cite>params</cite></p>
</dd>
<dt><strong>params: dict</strong></dt><dd><p>Dict with variational family parameters, full description can be found below.
Cannot be passed both with <cite>vfam</cite></p>
</dd>
<dt><strong>random_seed: int</strong></dt><dd><p>Random seed for underlying random generator</p>
</dd>
<dt><strong>model :</strong></dt><dd><p>PyMC3 Model</p>
</dd>
<dt><strong>local: bool</strong></dt><dd><p>Indicates whether this group is local. Cannot be passed without <cite>params</cite>.
Such group should have only one variable</p>
</dd>
<dt><strong>rowwise: bool</strong></dt><dd><p>Indicates whether this group is independently parametrized over first dim.
Such group should have only one variable</p>
</dd>
<dt><strong>options: dict</strong></dt><dd><p>Special options for the group</p>
</dd>
<dt><strong>kwargs: Other kwargs for the group</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Approximation</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>Group instance/class has some important constants:</p>
<ul>
<li><p><strong>supports_batched</strong>
Determines whether such variational family can be used for AEVB or rowwise approx.</p>
<p>AEVB approx is such approx that somehow depends on input data. It can be treated
as conditional distribution. You can see more about in the corresponding paper
mentioned in references.</p>
<p>Rowwise mode is a special case approximation that treats every ‘row’, of a tensor as
independent from each other. Some distributions can’t do that by
definition e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Empirical</span></code> that consists of particles only.</p>
</li>
<li><p><strong>has_logq</strong>
Tells that distribution is defined explicitly</p></li>
</ul>
<p>These constants help providing the correct inference method for given parametrization</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Kingma, D. P., &amp; Welling, M. (2014).
<a class="reference external" href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes. stat, 1050, 1.</a></p></li>
</ul>
<p class="rubric">Examples</p>
<p><strong>Basic Initialization</strong></p>
<p><a class="reference internal" href="#pymc3.variational.opvi.Group" title="pymc3.variational.opvi.Group"><code class="xref py py-class docutils literal notranslate"><span class="pre">Group</span></code></a> is a factory class. You do not need to call every ApproximationGroup explicitly.
Passing the correct <cite>vfam</cite> (Variational FAMily) argument you’ll tell what
parametrization is desired for the group. This helps not to overload code with lots of classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent1</span><span class="p">,</span> <span class="n">latent2</span><span class="p">],</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;mean_field&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The other way to select approximation is to provide <cite>params</cite> dictionary that has some
predefined well shaped parameters. Keys of the dict serve as an identifier for variational family and help
to autoselect the correct group class. To identify what approximation to use, params dict should
have the full set of needed parameters. As there are 2 ways to instantiate the <a class="reference internal" href="#pymc3.variational.opvi.Group" title="pymc3.variational.opvi.Group"><code class="xref py py-class docutils literal notranslate"><span class="pre">Group</span></code></a>
passing both <cite>vfam</cite> and <cite>params</cite> is prohibited. Partial parametrization is prohibited by design to
avoid corner cases and possible problems.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">my_mu</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="n">my_rho</span><span class="p">))</span>
</pre></div>
</div>
<p>Important to note that in case you pass custom params they will not be autocollected by optimizer, you’ll
have to provide them with <cite>more_obj_params</cite> keyword.</p>
<p><strong>Supported dict keys:</strong></p>
<ul>
<li><p><cite>{‘mu’, ‘rho’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">MeanFieldGroup</span></code></p></li>
<li><p><cite>{‘mu’, ‘L_tril’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">FullRankGroup</span></code></p></li>
<li><p><cite>{‘histogram’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">EmpiricalGroup</span></code></p></li>
<li><p><cite>{0, 1, 2, 3, …, k-1}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">NormalizingFlowGroup</span></code> of depth <cite>k</cite></p>
<p>NormalizingFlows have other parameters than ordinary groups and should be
passed as nested dicts with the following keys:</p>
<ul class="simple">
<li><p><cite>{‘u’, ‘w’, ‘b’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">PlanarFlow</span></code></p></li>
<li><p><cite>{‘a’, ‘b’, ‘z_ref’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">RadialFlow</span></code></p></li>
<li><p><cite>{‘loc’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">LocFlow</span></code></p></li>
<li><p><cite>{‘rho’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">ScaleFlow</span></code></p></li>
<li><p><cite>{‘v’}</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">HouseholderFlow</span></code></p></li>
</ul>
<p>Note that all integer keys should be present in the dictionary. An example
of NormalizingFlow initialization can be found below.</p>
</li>
</ul>
<p><strong>Using AEVB</strong></p>
<p>Autoencoding variational Bayes is a powerful tool to get conditional <span class="math notranslate nohighlight">\(q(\lambda|X)\)</span> distribution
on latent variables. It is well supported by PyMC3 and all you need is to provide a dictionary
with well shaped variational parameters, the correct approximation will be autoselected as mentioned
in section above. However we have some implementation restrictions in AEVB. They require autoencoded
variable to have first dimension as <em>batch</em> dimension and other dimensions should stay fixed.
With this assumptions it is possible to generalize all variational approximation families as
batched approximations that have flexible parameters and leading axis.</p>
<p>Only single variable local group is supported. Params are required.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># for mean field</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">my_mu</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="n">my_rho</span><span class="p">),</span> <span class="n">local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or for full rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">my_mu</span><span class="p">,</span> <span class="n">L_tril</span><span class="o">=</span><span class="n">my_L_tril</span><span class="p">),</span> <span class="n">local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>An Approximation class is selected automatically based on the keys in dict.</p></li>
<li><p><cite>my_mu</cite> and <cite>my_rho</cite> are usually estimated with neural network or function approximator.</p></li>
</ul>
<p><strong>Using Row-Wise Group</strong></p>
<p>Batch groups have independent row wise approximations, thus using batched
mean field will give no effect. It is more interesting if you want each row of a matrix
to be parametrized independently with normalizing flow or full rank gaussian.</p>
<p>To tell <a class="reference internal" href="#pymc3.variational.opvi.Group" title="pymc3.variational.opvi.Group"><code class="xref py py-class docutils literal notranslate"><span class="pre">Group</span></code></a> that group is batched you need set <cite>batched</cite> kwarg as <cite>True</cite>.
Only single variable group is allowed due to implementation details.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;fr&#39;</span><span class="p">,</span> <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># &#39;fr&#39; is alias for &#39;full_rank&#39;</span>
</pre></div>
</div>
<p>The resulting approximation for this variable will have the following structure</p>
<div class="math notranslate nohighlight">
\[latent3_{i, \dots} \sim \mathcal{N}(\mu_i, \Sigma_i) \forall i\]</div>
<p><strong>Note</strong>: Using rowwise and user-parametrized approximation is ok, but
shape should be checked beforehand, it is impossible to infer it by PyMC3</p>
<p><strong>Normalizing Flow Group</strong></p>
<p>In case you use simple initialization pattern using <cite>vfam</cite> you’ll not meet any changes.
Passing flow formula to <cite>vfam</cite> you’ll get correct flow parametrization for group</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;scale-hh*5-radial*4-loc&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note</strong>: Consider passing location flow as the last one and scale as the first one for stable inference.</p>
<p>Rowwise normalizing flow is supported as well</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;scale-hh*2-radial-loc&#39;</span><span class="p">,</span> <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Custom parameters for normalizing flow can be a real trouble for the first time.
They have quite different format from the rest variational families.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># int is used as key, it also tells the flow position</span>
<span class="gp">... </span><span class="n">flow_params</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="c1"># `rho` parametrizes scale flow, softplus is used to map (-inf; inf) -&gt; (0, inf)</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">rho</span><span class="o">=</span><span class="n">my_scale</span><span class="p">),</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">my_v1</span><span class="p">),</span>  <span class="c1"># Householder Flow, `v` is parameter name from the original paper</span>
<span class="gp">... </span>    <span class="mi">2</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">my_v2</span><span class="p">),</span>  <span class="c1"># do not miss any number in dict, or else error is raised</span>
<span class="gp">... </span>    <span class="mi">3</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">my_a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">my_b</span><span class="p">,</span> <span class="n">z_ref</span><span class="o">=</span><span class="n">my_z_ref</span><span class="p">),</span>  <span class="c1"># Radial flow</span>
<span class="gp">... </span>    <span class="mi">4</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">my_loc</span><span class="p">)</span>  <span class="c1"># Location Flow</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">... </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="n">flow_params</span><span class="p">)</span>
<span class="gp">... </span><span class="c1"># local=True can be added in case you do AEVB inference</span>
<span class="gp">... </span><span class="n">group</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent3</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="n">flow_params</span><span class="p">,</span> <span class="n">local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Delayed Initialization</strong></p>
<p>When you have a lot of latent variables it is impractical to do it all manually.
To make life much simpler, You can pass <cite>None</cite> instead of list of variables. That case
you’ll not create shared parameters until you pass all collected groups to
Approximation object that collects all the groups together and checks that every group is
correctly initialized. For those groups which have group equal to <cite>None</cite> it will collect all
the rest variables not covered by other groups and perform delayed init.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">group_1</span> <span class="o">=</span> <span class="n">Group</span><span class="p">([</span><span class="n">latent1</span><span class="p">],</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;fr&#39;</span><span class="p">)</span>  <span class="c1"># latent1 has full rank approximation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_other</span> <span class="o">=</span> <span class="n">Group</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">vfam</span><span class="o">=</span><span class="s1">&#39;mf&#39;</span><span class="p">)</span>  <span class="c1"># other variables have mean field Q</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">approx</span> <span class="o">=</span> <span class="n">Approximation</span><span class="p">([</span><span class="n">group_1</span><span class="p">,</span> <span class="n">group_other</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Summing Up</strong></p>
<p>When you have created all the groups they need to pass all the groups to <a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Approximation</span></code></a>.
It does not accept any other parameter rather than <cite>groups</cite></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">approx</span> <span class="o">=</span> <span class="n">Approximation</span><span class="p">(</span><span class="n">my_groups</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="pymc3.variational.opvi.Group.logq">
<em class="property">property </em><code class="sig-name descname">logq</code><a class="headerlink" href="#pymc3.variational.opvi.Group.logq" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - Monte Carlo estimate for group <cite>logQ</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.logq_norm">
<em class="property">property </em><code class="sig-name descname">logq_norm</code><a class="headerlink" href="#pymc3.variational.opvi.Group.logq_norm" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - Monte Carlo estimate for group <cite>logQ</cite> normalized</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.make_size_and_deterministic_replacements">
<code class="sig-name descname">make_size_and_deterministic_replacements</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">d</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group.make_size_and_deterministic_replacements" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - creates correct replacements for initial depending on
sample size and deterministic flag</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>s: scalar</strong></dt><dd><p>sample size</p>
</dd>
<dt><strong>d: bool or scalar</strong></dt><dd><p>whether sampling is done deterministically</p>
</dd>
<dt><strong>more_replacements: dict</strong></dt><dd><p>replacements for shape and initial</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>dict with replacements for initial</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.set_size_and_deterministic">
<code class="sig-name descname">set_size_and_deterministic</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em>, <em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">d</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group.set_size_and_deterministic" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - after node is sampled via <a class="reference internal" href="#pymc3.variational.opvi.Group.symbolic_sample_over_posterior" title="pymc3.variational.opvi.Group.symbolic_sample_over_posterior"><code class="xref py py-func docutils literal notranslate"><span class="pre">symbolic_sample_over_posterior()</span></code></a> or
<a class="reference internal" href="#pymc3.variational.opvi.Group.symbolic_single_sample" title="pymc3.variational.opvi.Group.symbolic_single_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">symbolic_single_sample()</span></code></a> new random generator can be allocated and applied to node</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>node: :class:`Variable`</strong></dt><dd><p>Theano node with symbolically applied VI replacements</p>
</dd>
<dt><strong>s: scalar</strong></dt><dd><p>desired number of samples</p>
</dd>
<dt><strong>d: bool or int</strong></dt><dd><p>whether sampling is done deterministically</p>
</dd>
<dt><strong>more_replacements: dict</strong></dt><dd><p>more replacements to apply</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> with applied replacements, ready to use</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_logq">
<em class="property">property </em><code class="sig-name descname">symbolic_logq</code><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_logq" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - correctly scaled <cite>self.symbolic_logq_not_scaled</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_logq_not_scaled">
<em class="property">property </em><code class="sig-name descname">symbolic_logq_not_scaled</code><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_logq_not_scaled" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - symbolically computed logq for <cite>self.symbolic_random</cite>
computations can be more efficient since all is known beforehand including
<cite>self.symbolic_random</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_normalizing_constant">
<em class="property">property </em><code class="sig-name descname">symbolic_normalizing_constant</code><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_normalizing_constant" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - normalizing constant for <cite>self.logq</cite>, scales it to <cite>minibatch_size</cite> instead of <cite>total_size</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_random">
<em class="property">property </em><code class="sig-name descname">symbolic_random</code><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_random" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - abstract node that takes <cite>self.symbolic_initial</cite> and creates
approximate posterior that is parametrized with <cite>self.params_dict</cite>.</p>
<p>Implementation should take in account <cite>self.batched</cite>. If <cite>self.batched</cite> is <cite>True</cite>, then
<cite>self.symbolic_initial</cite> is 3d tensor, else 2d</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt>tensor</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_random2d">
<em class="property">property </em><code class="sig-name descname">symbolic_random2d</code><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_random2d" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - <cite>self.symbolic_random</cite> flattened to matrix</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_sample_over_posterior">
<code class="sig-name descname">symbolic_sample_over_posterior</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_sample_over_posterior" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - performs sampling of node applying independent samples from posterior each time.
Note that it is done symbolically and this node needs <a class="reference internal" href="#pymc3.variational.opvi.Group.set_size_and_deterministic" title="pymc3.variational.opvi.Group.set_size_and_deterministic"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_size_and_deterministic()</span></code></a> call</p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.symbolic_single_sample">
<code class="sig-name descname">symbolic_single_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group.symbolic_single_sample" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - performs sampling of node applying single sample from posterior.
Note that it is done symbolically and this node needs
<a class="reference internal" href="#pymc3.variational.opvi.Group.set_size_and_deterministic" title="pymc3.variational.opvi.Group.set_size_and_deterministic"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_size_and_deterministic()</span></code></a> call with <cite>size=1</cite></p>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.Group.to_flat_input">
<code class="sig-name descname">to_flat_input</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Group.to_flat_input" title="Permalink to this definition">¶</a></dt>
<dd><p><em>Dev</em> - replace vars with flattened view stored in <cite>self.inputs</cite></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.opvi.ObjectiveFunction">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.opvi.</code><code class="sig-name descname">ObjectiveFunction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em>, <em class="sig-param"><span class="n">tf</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper class for construction loss and updates for variational inference</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>op</strong><span class="classifier"><a class="reference internal" href="#pymc3.variational.opvi.Operator" title="pymc3.variational.opvi.Operator"><code class="xref py py-class docutils literal notranslate"><span class="pre">Operator</span></code></a></span></dt><dd><p>OPVI Functional operator</p>
</dd>
<dt><strong>tf</strong><span class="classifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">TestFunction</span></code></span></dt><dd><p>OPVI TestFunction</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.score_function">
<code class="sig-name descname">score_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sc_n_mc</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">more_replacements</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">fn_kwargs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.score_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile scoring function that operates which takes no inputs and returns Loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>sc_n_mc: `int`</strong></dt><dd><p>number of scoring MC samples</p>
</dd>
<dt><strong>more_replacements:</strong></dt><dd><p>Apply custom replacements before compiling a function</p>
</dd>
<dt><strong>fn_kwargs: `dict`</strong></dt><dd><p>arbitrary kwargs passed to <cite>theano.function</cite></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>theano.function</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.step_function">
<code class="sig-name descname">step_function</code><span class="sig-paren">(</span><em class="sig-param">obj_n_mc=None</em>, <em class="sig-param">tf_n_mc=None</em>, <em class="sig-param">obj_optimizer=&lt;function adagrad_window&gt;</em>, <em class="sig-param">test_optimizer=&lt;function adagrad_window&gt;</em>, <em class="sig-param">more_obj_params=None</em>, <em class="sig-param">more_tf_params=None</em>, <em class="sig-param">more_updates=None</em>, <em class="sig-param">more_replacements=None</em>, <em class="sig-param">total_grad_norm_constraint=None</em>, <em class="sig-param">score=False</em>, <em class="sig-param">fn_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.step_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Step function that should be called on each optimization step.</p>
<p>Generally it solves the following problem:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\lambda^{\*}} = \inf_{\lambda} \sup_{\theta} t(\mathbb{E}_{\lambda}[(O^{p,q}f_{\theta})(z)])\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>obj_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of objective gradients</p>
</dd>
<dt><strong>tf_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of test function gradients</p>
</dd>
<dt><strong>obj_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for objective params</p>
</dd>
<dt><strong>test_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for test function params</p>
</dd>
<dt><strong>more_obj_params: `list`</strong></dt><dd><p>Add custom params for objective optimizer</p>
</dd>
<dt><strong>more_tf_params: `list`</strong></dt><dd><p>Add custom params for test function optimizer</p>
</dd>
<dt><strong>more_updates: `dict`</strong></dt><dd><p>Add custom updates to resulting updates</p>
</dd>
<dt><strong>total_grad_norm_constraint: `float`</strong></dt><dd><p>Bounds gradient norm, prevents exploding gradient problem</p>
</dd>
<dt><strong>score: `bool`</strong></dt><dd><p>calculate loss on each step? Defaults to False for speed</p>
</dd>
<dt><strong>fn_kwargs: `dict`</strong></dt><dd><p>Add kwargs to theano.function (e.g. <cite>{‘profile’: True}</cite>)</p>
</dd>
<dt><strong>more_replacements: `dict`</strong></dt><dd><p>Apply custom replacements before calculating gradients</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><cite>theano.function</cite></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.updates">
<code class="sig-name descname">updates</code><span class="sig-paren">(</span><em class="sig-param">obj_n_mc=None</em>, <em class="sig-param">tf_n_mc=None</em>, <em class="sig-param">obj_optimizer=&lt;function adagrad_window&gt;</em>, <em class="sig-param">test_optimizer=&lt;function adagrad_window&gt;</em>, <em class="sig-param">more_obj_params=None</em>, <em class="sig-param">more_tf_params=None</em>, <em class="sig-param">more_updates=None</em>, <em class="sig-param">more_replacements=None</em>, <em class="sig-param">total_grad_norm_constraint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate gradients for objective function, test function and then
constructs updates for optimization step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>obj_n_mc</strong><span class="classifier">int</span></dt><dd><p>Number of monte carlo samples used for approximation of objective gradients</p>
</dd>
<dt><strong>tf_n_mc</strong><span class="classifier">int</span></dt><dd><p>Number of monte carlo samples used for approximation of test function gradients</p>
</dd>
<dt><strong>obj_optimizer</strong><span class="classifier">function (loss, params) -&gt; updates</span></dt><dd><p>Optimizer that is used for objective params</p>
</dd>
<dt><strong>test_optimizer</strong><span class="classifier">function (loss, params) -&gt; updates</span></dt><dd><p>Optimizer that is used for test function params</p>
</dd>
<dt><strong>more_obj_params</strong><span class="classifier">list</span></dt><dd><p>Add custom params for objective optimizer</p>
</dd>
<dt><strong>more_tf_params</strong><span class="classifier">list</span></dt><dd><p>Add custom params for test function optimizer</p>
</dd>
<dt><strong>more_updates</strong><span class="classifier">dict</span></dt><dd><p>Add custom updates to resulting updates</p>
</dd>
<dt><strong>more_replacements</strong><span class="classifier">dict</span></dt><dd><p>Apply custom replacements before calculating gradients</p>
</dd>
<dt><strong>total_grad_norm_constraint</strong><span class="classifier">float</span></dt><dd><p>Bounds gradient norm, prevents exploding gradient problem</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">ObjectiveUpdates</span></code></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.opvi.Operator">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.opvi.</code><code class="sig-name descname">Operator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">approx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Operator" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Base class for Operator</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>an approximation instance</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For implementing custom operator it is needed to define <a class="reference internal" href="#pymc3.variational.opvi.Operator.apply" title="pymc3.variational.opvi.Operator.apply"><code class="xref py py-func docutils literal notranslate"><span class="pre">Operator.apply()</span></code></a> method</p>
<dl class="py method">
<dt id="pymc3.variational.opvi.Operator.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Operator.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator itself</p>
<div class="math notranslate nohighlight">
\[(O^{p,q}f_{\theta})(z)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>f: :class:`TestFunction` or None</strong></dt><dd><p>function that takes <cite>z = self.input</cite> and returns
same dimensional output</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>TensorVariable</dt><dd><p>symbolically applied operator</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pymc3.variational.opvi.Operator.objective_class">
<code class="sig-name descname">objective_class</code><a class="headerlink" href="#pymc3.variational.opvi.Operator.objective_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#pymc3.variational.opvi.ObjectiveFunction" title="pymc3.variational.opvi.ObjectiveFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">pymc3.variational.opvi.ObjectiveFunction</span></code></a></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pymc3.variational.inference">
<span id="vi-inference-api"></span><h3>VI Inference API<a class="headerlink" href="#module-pymc3.variational.inference" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="pymc3.variational.inference.ADVI">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">ADVI</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ADVI" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Automatic Differentiation Variational Inference (ADVI)</strong></p>
<p>This class implements the meanfield ADVI, where the variational
posterior distribution is assumed to be spherical Gaussian without
correlation of parameters and fit to the true posterior distribution.
The means and standard deviations of the variational posterior are referred
to as variational parameters.</p>
<p>For explanation, we classify random variables in probabilistic models into
three types. Observed random variables
<span class="math notranslate nohighlight">\({\cal Y}=\{\mathbf{y}_{i}\}_{i=1}^{N}\)</span> are <span class="math notranslate nohighlight">\(N\)</span> observations.
Each <span class="math notranslate nohighlight">\(\mathbf{y}_{i}\)</span> can be a set of observed random variables,
i.e., <span class="math notranslate nohighlight">\(\mathbf{y}_{i}=\{\mathbf{y}_{i}^{k}\}_{k=1}^{V_{o}}\)</span>, where
<span class="math notranslate nohighlight">\(V_{k}\)</span> is the number of the types of observed random variables
in the model.</p>
<p>The next ones are global random variables
<span class="math notranslate nohighlight">\(\Theta=\{\theta^{k}\}_{k=1}^{V_{g}}\)</span>, which are used to calculate
the probabilities for all observed samples.</p>
<p>The last ones are local random variables
<span class="math notranslate nohighlight">\({\cal Z}=\{\mathbf{z}_{i}\}_{i=1}^{N}\)</span>, where
<span class="math notranslate nohighlight">\(\mathbf{z}_{i}=\{\mathbf{z}_{i}^{k}\}_{k=1}^{V_{l}}\)</span>.
These RVs are used only in AEVB.</p>
<p>The goal of ADVI is to approximate the posterior distribution
<span class="math notranslate nohighlight">\(p(\Theta,{\cal Z}|{\cal Y})\)</span> by variational posterior
<span class="math notranslate nohighlight">\(q(\Theta)\prod_{i=1}^{N}q(\mathbf{z}_{i})\)</span>. All of these terms
are normal distributions (mean-field approximation).</p>
<p><span class="math notranslate nohighlight">\(q(\Theta)\)</span> is parametrized with its means and standard deviations.
These parameters are denoted as <span class="math notranslate nohighlight">\(\gamma\)</span>. While <span class="math notranslate nohighlight">\(\gamma\)</span> is
a constant, the parameters of <span class="math notranslate nohighlight">\(q(\mathbf{z}_{i})\)</span> are dependent on
each observation. Therefore these parameters are denoted as
<span class="math notranslate nohighlight">\(\xi(\mathbf{y}_{i}; \nu)\)</span>, where <span class="math notranslate nohighlight">\(\nu\)</span> is the parameters
of <span class="math notranslate nohighlight">\(\xi(\cdot)\)</span>. For example, <span class="math notranslate nohighlight">\(\xi(\cdot)\)</span> can be a
multilayer perceptron or convolutional neural network.</p>
<p>In addition to <span class="math notranslate nohighlight">\(\xi(\cdot)\)</span>, we can also include deterministic
mappings for the likelihood of observations. We denote the parameters of
the deterministic mappings as <span class="math notranslate nohighlight">\(\eta\)</span>. An example of such mappings is
the deconvolutional neural network used in the convolutional VAE example
in the PyMC3 notebook directory.</p>
<p>This function maximizes the evidence lower bound (ELBO)
<span class="math notranslate nohighlight">\({\cal L}(\gamma, \nu, \eta)\)</span> defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}{\cal L}(\gamma,\nu,\eta) &amp; =
\mathbf{c}_{o}\mathbb{E}_{q(\Theta)}\left[
\sum_{i=1}^{N}\mathbb{E}_{q(\mathbf{z}_{i})}\left[
\log p(\mathbf{y}_{i}|\mathbf{z}_{i},\Theta,\eta)
\right]\right] \\ &amp;
- \mathbf{c}_{g}KL\left[q(\Theta)||p(\Theta)\right]
- \mathbf{c}_{l}\sum_{i=1}^{N}
    KL\left[q(\mathbf{z}_{i})||p(\mathbf{z}_{i})\right],\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(KL[q(v)||p(v)]\)</span> is the Kullback-Leibler divergence</p>
<div class="math notranslate nohighlight">
\[KL[q(v)||p(v)] = \int q(v)\log\frac{q(v)}{p(v)}dv,\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{c}_{o/g/l}\)</span> are vectors for weighting each term of ELBO.
More precisely, we can write each of the terms in ELBO as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{c}_{o}\log p(\mathbf{y}_{i}|\mathbf{z}_{i},\Theta,\eta) &amp; = &amp;
\sum_{k=1}^{V_{o}}c_{o}^{k}
    \log p(\mathbf{y}_{i}^{k}|
           {\rm pa}(\mathbf{y}_{i}^{k},\Theta,\eta)) \\
\mathbf{c}_{g}KL\left[q(\Theta)||p(\Theta)\right] &amp; = &amp;
\sum_{k=1}^{V_{g}}c_{g}^{k}KL\left[
    q(\theta^{k})||p(\theta^{k}|{\rm pa(\theta^{k})})\right] \\
\mathbf{c}_{l}KL\left[q(\mathbf{z}_{i}||p(\mathbf{z}_{i})\right] &amp; = &amp;
\sum_{k=1}^{V_{l}}c_{l}^{k}KL\left[
    q(\mathbf{z}_{i}^{k})||
    p(\mathbf{z}_{i}^{k}|{\rm pa}(\mathbf{z}_{i}^{k}))\right],\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\({\rm pa}(v)\)</span> denotes the set of parent variables of <span class="math notranslate nohighlight">\(v\)</span>
in the directed acyclic graph of the model.</p>
<p>When using mini-batches, <span class="math notranslate nohighlight">\(c_{o}^{k}\)</span> and <span class="math notranslate nohighlight">\(c_{l}^{k}\)</span> should be
set to <span class="math notranslate nohighlight">\(N/M\)</span>, where <span class="math notranslate nohighlight">\(M\)</span> is the number of observations in each
mini-batch. This is done with supplying <cite>total_size</cite> parameter to
observed nodes (e.g. <code class="code docutils literal notranslate"><span class="pre">Normal('x',</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">observed=data,</span> <span class="pre">total_size=10000)</span></code>).
In this case it is possible to automatically determine appropriate scaling for <span class="math notranslate nohighlight">\(logp\)</span>
of observed nodes. Interesting to note that it is possible to have two independent
observed variables with different <cite>total_size</cite> and iterate them independently
during inference.</p>
<p>For working with ADVI, we need to give</p>
<ul>
<li><p>The probabilistic model</p>
<p><cite>model</cite> with three types of RVs (<cite>observed_RVs</cite>,
<cite>global_RVs</cite> and <cite>local_RVs</cite>).</p>
</li>
<li><p>(optional) Minibatches</p>
<p>The tensors to which mini-bathced samples are supplied are
handled separately by using callbacks in <a class="reference internal" href="#pymc3.variational.inference.Inference.fit" title="pymc3.variational.inference.Inference.fit"><code class="xref py py-func docutils literal notranslate"><span class="pre">Inference.fit()</span></code></a> method
that change storage of shared theano variable or by <code class="xref py py-func docutils literal notranslate"><span class="pre">pymc3.generator()</span></code>
that automatically iterates over minibatches and defined beforehand.</p>
</li>
<li><p>(optional) Parameters of deterministic mappings</p>
<p>They have to be passed along with other params to <a class="reference internal" href="#pymc3.variational.inference.Inference.fit" title="pymc3.variational.inference.Inference.fit"><code class="xref py py-func docutils literal notranslate"><span class="pre">Inference.fit()</span></code></a> method
as <cite>more_obj_params</cite> argument.</p>
</li>
</ul>
<p>For more information concerning training stage please reference
<a class="reference internal" href="#pymc3.variational.opvi.ObjectiveFunction.step_function" title="pymc3.variational.opvi.ObjectiveFunction.step_function"><code class="xref py py-func docutils literal notranslate"><span class="pre">pymc3.variational.opvi.ObjectiveFunction.step_function()</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>local_rv: dict[var-&gt;tuple]</strong></dt><dd><p>mapping {model_variable -&gt; approx params}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</p>
</dd>
<dt><strong>model: :class:`pymc3.Model`</strong></dt><dd><p>PyMC3 model for inference</p>
</dd>
<dt><strong>random_seed: None or int</strong></dt><dd><p>leave None to use package global RandomStream or other
valid value to create instance specific one</p>
</dd>
<dt><strong>start: `Point`</strong></dt><dd><p>starting point for inference</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A.,
and Blei, D. M. (2016). Automatic Differentiation Variational
Inference. arXiv preprint arXiv:1603.00788.</p></li>
<li><p>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</p></li>
<li><p>Kingma, D. P., &amp; Welling, M. (2014).
Auto-Encoding Variational Bayes. stat, 1050, 1.</p></li>
</ul>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.ASVGD">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">ASVGD</code><span class="sig-paren">(</span><em class="sig-param">approx=None</em>, <em class="sig-param">estimator=&lt;class 'pymc3.variational.operators.KSD'&gt;</em>, <em class="sig-param">kernel=&lt;pymc3.variational.test_functions.RBF object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ASVGD" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Amortized Stein Variational Gradient Descent</strong></p>
<p><strong>not suggested to use</strong></p>
<p>This inference is based on Kernelized Stein Discrepancy
it’s main idea is to move initial noisy particles so that
they fit target distribution best.</p>
<p>Algorithm is outlined below</p>
<p><em>Input:</em> Parametrized random generator <span class="math notranslate nohighlight">\(R_{\theta}\)</span></p>
<p><em>Output:</em> <span class="math notranslate nohighlight">\(R_{\theta^{*}}\)</span> that approximates the target distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Delta x_i &amp;= \hat{\phi}^{*}(x_i) \\
\hat{\phi}^{*}(x) &amp;= \frac{1}{n}\sum^{n}_{j=1}[k(x_j,x) \nabla_{x_j} logp(x_j)+ \nabla_{x_j} k(x_j,x)] \\
\Delta_{\theta} &amp;= \frac{1}{n}\sum^{n}_{i=1}\Delta x_i\frac{\partial x_i}{\partial \theta}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>default is <code class="xref py py-class docutils literal notranslate"><span class="pre">FullRank</span></code> but can be any</p>
</dd>
<dt><strong>kernel: `callable`</strong></dt><dd><p>kernel function for KSD <span class="math notranslate nohighlight">\(f(histogram) -&gt; (k(x,.), \nabla_x k(x,.))\)</span></p>
</dd>
<dt><strong>model: :class:`Model`</strong></dt><dd></dd>
<dt><strong>kwargs: kwargs for gradient estimator</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Dilin Wang, Yihao Feng, Qiang Liu (2016)
Learning to Sample Using Stein Discrepancy
<a class="reference external" href="http://bayesiandeeplearning.org/papers/BDL_21.pdf">http://bayesiandeeplearning.org/papers/BDL_21.pdf</a></p></li>
<li><p>Dilin Wang, Qiang Liu (2016)
Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning
arXiv:1611.01722</p></li>
<li><p>Yang Liu, Prajit Ramachandran, Qiang Liu, Jian Peng (2017)
Stein Variational Policy Gradient
arXiv:1704.02399</p></li>
</ul>
<dl class="py method">
<dt id="pymc3.variational.inference.ASVGD.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n</span><span class="o">=</span><span class="default_value">10000</span></em>, <em class="sig-param"><span class="n">score</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">callbacks</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">obj_n_mc</span><span class="o">=</span><span class="default_value">500</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ASVGD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Operator Variational Inference</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n: int</strong></dt><dd><p>number of iterations</p>
</dd>
<dt><strong>score: bool</strong></dt><dd><p>evaluate loss on each iteration or not</p>
</dd>
<dt><strong>callbacks: list[function: (Approximation, losses, i) -&gt; None]</strong></dt><dd><p>calls provided functions after each iteration step</p>
</dd>
<dt><strong>progressbar: bool</strong></dt><dd><p>whether to show progressbar or not</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Approximation</span></code></dt><dd></dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>obj_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of objective gradients</p>
</dd>
<dt><strong>tf_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of test function gradients</p>
</dd>
<dt><strong>obj_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for objective params</p>
</dd>
<dt><strong>test_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for test function params</p>
</dd>
<dt><strong>more_obj_params: `list`</strong></dt><dd><p>Add custom params for objective optimizer</p>
</dd>
<dt><strong>more_tf_params: `list`</strong></dt><dd><p>Add custom params for test function optimizer</p>
</dd>
<dt><strong>more_updates: `dict`</strong></dt><dd><p>Add custom updates to resulting updates</p>
</dd>
<dt><strong>total_grad_norm_constraint: `float`</strong></dt><dd><p>Bounds gradient norm, prevents exploding gradient problem</p>
</dd>
<dt><strong>fn_kwargs: `dict`</strong></dt><dd><p>Add kwargs to theano.function (e.g. <cite>{‘profile’: True}</cite>)</p>
</dd>
<dt><strong>more_replacements: `dict`</strong></dt><dd><p>Apply custom replacements before calculating gradients</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.FullRankADVI">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">FullRankADVI</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.FullRankADVI" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Full Rank Automatic Differentiation Variational Inference (ADVI)</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>local_rv: dict[var-&gt;tuple]</strong></dt><dd><p>mapping {model_variable -&gt; approx params}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</p>
</dd>
<dt><strong>model: :class:`pymc3.Model`</strong></dt><dd><p>PyMC3 model for inference</p>
</dd>
<dt><strong>random_seed: None or int</strong></dt><dd><p>leave None to use package global RandomStream or other
valid value to create instance specific one</p>
</dd>
<dt><strong>start: `Point`</strong></dt><dd><p>starting point for inference</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A.,
and Blei, D. M. (2016). Automatic Differentiation Variational
Inference. arXiv preprint arXiv:1603.00788.</p></li>
<li><p>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</p></li>
<li><p>Kingma, D. P., &amp; Welling, M. (2014).
Auto-Encoding Variational Bayes. stat, 1050, 1.</p></li>
</ul>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.ImplicitGradient">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">ImplicitGradient</code><span class="sig-paren">(</span><em class="sig-param">approx</em>, <em class="sig-param">estimator=&lt;class 'pymc3.variational.operators.KSD'&gt;</em>, <em class="sig-param">kernel=&lt;pymc3.variational.test_functions.RBF object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ImplicitGradient" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Implicit Gradient for Variational Inference</strong></p>
<p><strong>not suggested to use</strong></p>
<p>An approach to fit arbitrary approximation by computing kernel based gradient
By default RBF kernel is used for gradient estimation. Default estimator is
Kernelized Stein Discrepancy with temperature equal to 1. This temperature works
only for large number of samples. Larger temperature is needed for small number of
samples but there is no theoretical approach to choose the best one in such case.</p>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.Inference">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">Inference</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em>, <em class="sig-param"><span class="n">approx</span></em>, <em class="sig-param"><span class="n">tf</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.Inference" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Base class for Variational Inference</strong></p>
<p>Communicates Operator, Approximation and Test Function to build Objective Function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>op: Operator class</strong></dt><dd></dd>
<dt><strong>approx: Approximation class or instance</strong></dt><dd></dd>
<dt><strong>tf: TestFunction instance</strong></dt><dd></dd>
<dt><strong>model: Model</strong></dt><dd><p>PyMC3 Model</p>
</dd>
<dt><strong>kwargs: kwargs passed to :class:`Operator`</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.variational.inference.Inference.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n</span><span class="o">=</span><span class="default_value">10000</span></em>, <em class="sig-param"><span class="n">score</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">callbacks</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.Inference.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Operator Variational Inference</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n: int</strong></dt><dd><p>number of iterations</p>
</dd>
<dt><strong>score: bool</strong></dt><dd><p>evaluate loss on each iteration or not</p>
</dd>
<dt><strong>callbacks: list[function: (Approximation, losses, i) -&gt; None]</strong></dt><dd><p>calls provided functions after each iteration step</p>
</dd>
<dt><strong>progressbar: bool</strong></dt><dd><p>whether to show progressbar or not</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Approximation</span></code></dt><dd></dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>obj_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of objective gradients</p>
</dd>
<dt><strong>tf_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of test function gradients</p>
</dd>
<dt><strong>obj_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for objective params</p>
</dd>
<dt><strong>test_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for test function params</p>
</dd>
<dt><strong>more_obj_params: `list`</strong></dt><dd><p>Add custom params for objective optimizer</p>
</dd>
<dt><strong>more_tf_params: `list`</strong></dt><dd><p>Add custom params for test function optimizer</p>
</dd>
<dt><strong>more_updates: `dict`</strong></dt><dd><p>Add custom updates to resulting updates</p>
</dd>
<dt><strong>total_grad_norm_constraint: `float`</strong></dt><dd><p>Bounds gradient norm, prevents exploding gradient problem</p>
</dd>
<dt><strong>fn_kwargs: `dict`</strong></dt><dd><p>Add kwargs to theano.function (e.g. <cite>{‘profile’: True}</cite>)</p>
</dd>
<dt><strong>more_replacements: `dict`</strong></dt><dd><p>Apply custom replacements before calculating gradients</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pymc3.variational.inference.Inference.refine">
<code class="sig-name descname">refine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n</span></em>, <em class="sig-param"><span class="n">progressbar</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.Inference.refine" title="Permalink to this definition">¶</a></dt>
<dd><p>Refine the solution using the last compiled step function</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.KLqp">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">KLqp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">approx</span></em>, <em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.KLqp" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Kullback Leibler Divergence Inference</strong></p>
<p>General approach to fit Approximations that define <span class="math notranslate nohighlight">\(logq\)</span>
by maximizing ELBO (Evidence Lower Bound). In some cases
rescaling the regularization term KL may be beneficial</p>
<div class="math notranslate nohighlight">
\[ELBO_\beta = \log p(D|\theta) - \beta KL(q||p)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>Approximation to fit, it is required to have <cite>logQ</cite></p>
</dd>
<dt><strong>beta: float</strong></dt><dd><p>Scales the regularization term in ELBO (see Christopher P. Burgess et al., 2017)</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Christopher P. Burgess et al. (NIPS, 2017)
Understanding disentangling in <span class="math notranslate nohighlight">\(\beta\)</span>-VAE
arXiv preprint 1804.03599</p></li>
</ul>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.NFVI">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">NFVI</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.NFVI" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Normalizing Flow based :class:`KLqp` inference</strong></p>
<p>Normalizing flow is a series of invertible transformations on initial distribution.</p>
<div class="math notranslate nohighlight">
\[z_K = f_K \circ \dots \circ f_2 \circ f_1(z_0)\]</div>
<p>In that case we can compute tractable density for the flow.</p>
<div class="math notranslate nohighlight">
\[\ln q_K(z_K) = \ln q_0(z_0) - \sum_{k=1}^{K}\ln \left|\frac{\partial f_k}{\partial z_{k-1}}\right|\]</div>
<p>Every <span class="math notranslate nohighlight">\(f_k\)</span> here is a parametric function with defined determinant.
We can choose every step here. For example the here is a simple flow
is an affine transform:</p>
<div class="math notranslate nohighlight">
\[z = loc(scale(z_0)) = \mu + \sigma * z_0\]</div>
<p>Here we get mean field approximation if <span class="math notranslate nohighlight">\(z_0 \sim \mathcal{N}(0, 1)\)</span></p>
<p><strong>Flow Formulas</strong></p>
<p>In PyMC3 there is a flexible way to define flows with formulas. We have 5 of them by the moment:</p>
<ul class="simple">
<li><p>Loc (<code class="code docutils literal notranslate"><span class="pre">loc</span></code>): <span class="math notranslate nohighlight">\(z' = z + \mu\)</span></p></li>
<li><p>Scale (<code class="code docutils literal notranslate"><span class="pre">scale</span></code>): <span class="math notranslate nohighlight">\(z' = \sigma * z\)</span></p></li>
<li><p>Planar (<code class="code docutils literal notranslate"><span class="pre">planar</span></code>): <span class="math notranslate nohighlight">\(z' = z + u * \tanh(w^T z + b)\)</span></p></li>
<li><p>Radial (<code class="code docutils literal notranslate"><span class="pre">radial</span></code>): <span class="math notranslate nohighlight">\(z' = z + \beta (\alpha + (z-z_r))^{-1}(z-z_r)\)</span></p></li>
<li><p>Householder (<code class="code docutils literal notranslate"><span class="pre">hh</span></code>): <span class="math notranslate nohighlight">\(z' = H z\)</span></p></li>
</ul>
<p>Formula can be written as a string, e.g. <cite>‘scale-loc’</cite>, <cite>‘scale-hh*4-loc’</cite>, <cite>‘panar*10’</cite>.
Every step is separated with <cite>‘-‘</cite>, repeated flow is marked with <cite>‘*’</cite> producing <cite>‘flow*repeats’</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>flow: str|AbstractFlow</strong></dt><dd><p>formula or initialized Flow, default is <cite>‘scale-loc’</cite> that
is identical to MeanField</p>
</dd>
<dt><strong>model: :class:`pymc3.Model`</strong></dt><dd><p>PyMC3 model for inference</p>
</dd>
<dt><strong>random_seed: None or int</strong></dt><dd><p>leave None to use package global RandomStream or other
valid value to create instance specific one</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.inference.SVGD">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">SVGD</code><span class="sig-paren">(</span><em class="sig-param">n_particles=100</em>, <em class="sig-param">jitter=1</em>, <em class="sig-param">model=None</em>, <em class="sig-param">start=None</em>, <em class="sig-param">random_seed=None</em>, <em class="sig-param">estimator=&lt;class 'pymc3.variational.operators.KSD'&gt;</em>, <em class="sig-param">kernel=&lt;pymc3.variational.test_functions.RBF object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.SVGD" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Stein Variational Gradient Descent</strong></p>
<p>This inference is based on Kernelized Stein Discrepancy
it’s main idea is to move initial noisy particles so that
they fit target distribution best.</p>
<p>Algorithm is outlined below</p>
<dl class="simple">
<dt><em>Input:</em> A target distribution with density function <span class="math notranslate nohighlight">\(p(x)\)</span></dt><dd><p>and a set of initial particles <span class="math notranslate nohighlight">\(\{x^0_i\}^n_{i=1}\)</span></p>
</dd>
</dl>
<p><em>Output:</em> A set of particles <span class="math notranslate nohighlight">\(\{x^{*}_i\}^n_{i=1}\)</span> that approximates the target distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}x_i^{l+1} &amp;\leftarrow x_i^{l} + \epsilon_l \hat{\phi}^{*}(x_i^l) \\
\hat{\phi}^{*}(x) &amp;= \frac{1}{n}\sum^{n}_{j=1}[k(x^l_j,x) \nabla_{x^l_j} logp(x^l_j)+ \nabla_{x^l_j} k(x^l_j,x)]\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_particles: `int`</strong></dt><dd><p>number of particles to use for approximation</p>
</dd>
<dt><strong>jitter: `float`</strong></dt><dd><p>noise sd for initial point</p>
</dd>
<dt><strong>model: :class:`pymc3.Model`</strong></dt><dd><p>PyMC3 model for inference</p>
</dd>
<dt><strong>kernel: `callable`</strong></dt><dd><p>kernel function for KSD <span class="math notranslate nohighlight">\(f(histogram) -&gt; (k(x,.), \nabla_x k(x,.))\)</span></p>
</dd>
<dt><strong>temperature: float</strong></dt><dd><p>parameter responsible for exploration, higher temperature gives more broad posterior estimate</p>
</dd>
<dt><strong>start: `dict`</strong></dt><dd><p>initial point for inference</p>
</dd>
<dt><strong>random_seed: None or int</strong></dt><dd><p>leave None to use package global RandomStream or other
valid value to create instance specific one</p>
</dd>
<dt><strong>start: `Point`</strong></dt><dd><p>starting point for inference</p>
</dd>
<dt><strong>kwargs: other keyword arguments passed to estimator</strong></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Qiang Liu, Dilin Wang (2016)
Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm
arXiv:1608.04471</p></li>
<li><p>Yang Liu, Prajit Ramachandran, Qiang Liu, Jian Peng (2017)
Stein Variational Policy Gradient
arXiv:1704.02399</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt id="pymc3.variational.inference.fit">
<code class="sig-prename descclassname">pymc3.variational.inference.</code><code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n</span><span class="o">=</span><span class="default_value">10000</span></em>, <em class="sig-param"><span class="n">local_rv</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">'advi'</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">random_seed</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inf_kwargs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Handy shortcut for using inference methods in functional way</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n: `int`</strong></dt><dd><p>number of iterations</p>
</dd>
<dt><strong>local_rv: dict[var-&gt;tuple]</strong></dt><dd><p>mapping {model_variable -&gt; approx params}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</p>
</dd>
<dt><strong>method: str or :class:`Inference`</strong></dt><dd><p>string name is case insensitive in:</p>
<ul class="simple">
<li><p>‘advi’  for ADVI</p></li>
<li><p>‘fullrank_advi’  for FullRankADVI</p></li>
<li><p>‘svgd’  for Stein Variational Gradient Descent</p></li>
<li><p>‘asvgd’  for Amortized Stein Variational Gradient Descent</p></li>
<li><p>‘nfvi’  for Normalizing Flow with default <cite>scale-loc</cite> flow</p></li>
<li><p>‘nfvi=&lt;formula&gt;’  for Normalizing Flow using formula</p></li>
</ul>
</dd>
<dt><strong>model: :class:`Model`</strong></dt><dd><p>PyMC3 model for inference</p>
</dd>
<dt><strong>random_seed: None or int</strong></dt><dd><p>leave None to use package global RandomStream or other
valid value to create instance specific one</p>
</dd>
<dt><strong>inf_kwargs: dict</strong></dt><dd><p>additional kwargs passed to <a class="reference internal" href="#pymc3.variational.inference.Inference" title="pymc3.variational.inference.Inference"><code class="xref py py-class docutils literal notranslate"><span class="pre">Inference</span></code></a></p>
</dd>
<dt><strong>start: `Point`</strong></dt><dd><p>starting point for inference</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Approximation</span></code></dt><dd></dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>score: bool</strong></dt><dd><p>evaluate loss on each iteration or not</p>
</dd>
<dt><strong>callbacks: list[function: (Approximation, losses, i) -&gt; None]</strong></dt><dd><p>calls provided functions after each iteration step</p>
</dd>
<dt><strong>progressbar: bool</strong></dt><dd><p>whether to show progressbar or not</p>
</dd>
<dt><strong>obj_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of objective gradients</p>
</dd>
<dt><strong>tf_n_mc: `int`</strong></dt><dd><p>Number of monte carlo samples used for approximation of test function gradients</p>
</dd>
<dt><strong>obj_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for objective params</p>
</dd>
<dt><strong>test_optimizer: function (grads, params) -&gt; updates</strong></dt><dd><p>Optimizer that is used for test function params</p>
</dd>
<dt><strong>more_obj_params: `list`</strong></dt><dd><p>Add custom params for objective optimizer</p>
</dd>
<dt><strong>more_tf_params: `list`</strong></dt><dd><p>Add custom params for test function optimizer</p>
</dd>
<dt><strong>more_updates: `dict`</strong></dt><dd><p>Add custom updates to resulting updates</p>
</dd>
<dt><strong>total_grad_norm_constraint: `float`</strong></dt><dd><p>Bounds gradient norm, prevents exploding gradient problem</p>
</dd>
<dt><strong>fn_kwargs: `dict`</strong></dt><dd><p>Add kwargs to theano.function (e.g. <cite>{‘profile’: True}</cite>)</p>
</dd>
<dt><strong>more_replacements: `dict`</strong></dt><dd><p>Apply custom replacements before calculating gradients</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pymc3.variational.approximations">
<span id="approximations"></span><h3>Approximations<a class="headerlink" href="#module-pymc3.variational.approximations" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="pymc3.variational.approximations.Empirical">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.approximations.</code><code class="sig-name descname">Empirical</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">trace</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.Empirical" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Single Group Full Rank Approximation</strong></p>
<p>Builds Approximation instance from a given trace,
it has the same interface as variational approximation</p>
<dl class="py method">
<dt id="pymc3.variational.approximations.Empirical.evaluate_over_trace">
<code class="sig-name descname">evaluate_over_trace</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">node</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.Empirical.evaluate_over_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows to statically evaluate any symbolic expression over the trace.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>node: Theano Variables (or Theano expressions)</strong></dt><dd></dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>evaluated node(s) over the posterior trace contained in the empirical approximation</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.approximations.FullRank">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.approximations.</code><code class="sig-name descname">FullRank</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.FullRank" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Single Group Full Rank Approximation</strong></p>
<p>Full Rank approximation to the posterior where Multivariate Gaussian family
is fitted to minimize KL divergence from True posterior. In contrast to
MeanField approach correlations between variables are taken in account. The
main drawback of the method is computational cost.</p>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.approximations.MeanField">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.approximations.</code><code class="sig-name descname">MeanField</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.MeanField" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Single Group Mean Field Approximation</strong></p>
<p>Mean Field approximation to the posterior where spherical Gaussian family
is fitted to minimize KL divergence from True posterior. It is assumed
that latent space variables are uncorrelated that is the main drawback
of the method</p>
</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.approximations.NormalizingFlow">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.approximations.</code><code class="sig-name descname">NormalizingFlow</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">flow</span><span class="o">=</span><span class="default_value">'scale-loc'</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.NormalizingFlow" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Single Group Normalizing Flow Approximation</strong></p>
<p>Normalizing flow is a series of invertible transformations on initial distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}z_K &amp;= f_K \circ \dots \circ f_2 \circ f_1(z_0) \\
&amp; z_0 \sim \mathcal{N}(0, 1)\end{split}\]</div>
<p>In that case we can compute tractable density for the flow.</p>
<div class="math notranslate nohighlight">
\[\ln q_K(z_K) = \ln q_0(z_0) - \sum_{k=1}^{K}\ln \left|\frac{\partial f_k}{\partial z_{k-1}}\right|\]</div>
<p>Every <span class="math notranslate nohighlight">\(f_k\)</span> here is a parametric function with defined determinant.
We can choose every step here. For example the here is a simple flow
is an affine transform:</p>
<div class="math notranslate nohighlight">
\[z = loc(scale(z_0)) = \mu + \sigma * z_0\]</div>
<p>Here we get mean field approximation if <span class="math notranslate nohighlight">\(z_0 \sim \mathcal{N}(0, 1)\)</span></p>
<p><strong>Flow Formulas</strong></p>
<p>In PyMC3 there is a flexible way to define flows with formulas. We have 5 of them by the moment:</p>
<ul class="simple">
<li><p>Loc (<code class="code docutils literal notranslate"><span class="pre">loc</span></code>): <span class="math notranslate nohighlight">\(z' = z + \mu\)</span></p></li>
<li><p>Scale (<code class="code docutils literal notranslate"><span class="pre">scale</span></code>): <span class="math notranslate nohighlight">\(z' = \sigma * z\)</span></p></li>
<li><p>Planar (<code class="code docutils literal notranslate"><span class="pre">planar</span></code>): <span class="math notranslate nohighlight">\(z' = z + u * \tanh(w^T z + b)\)</span></p></li>
<li><p>Radial (<code class="code docutils literal notranslate"><span class="pre">radial</span></code>): <span class="math notranslate nohighlight">\(z' = z + \beta (\alpha + (z-z_r))^{-1}(z-z_r)\)</span></p></li>
<li><p>Householder (<code class="code docutils literal notranslate"><span class="pre">hh</span></code>): <span class="math notranslate nohighlight">\(z' = H z\)</span></p></li>
</ul>
<p>Formula can be written as a string, e.g. <cite>‘scale-loc’</cite>, <cite>‘scale-hh*4-loc’</cite>, <cite>‘panar*10’</cite>.
Every step is separated with <cite>‘-‘</cite>, repeated flow is marked with <cite>‘*’</cite> producing <cite>‘flow*repeats’</cite>.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Danilo Jimenez Rezende, Shakir Mohamed, 2015
Variational Inference with Normalizing Flows
arXiv:1505.05770</p></li>
<li><p>Jakub M. Tomczak, Max Welling, 2016
Improving Variational Auto-Encoders using Householder Flow
arXiv:1611.09630</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt id="pymc3.variational.approximations.sample_approx">
<code class="sig-prename descclassname">pymc3.variational.approximations.</code><code class="sig-name descname">sample_approx</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">approx</span></em>, <em class="sig-param"><span class="n">draws</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">include_transformed</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.sample_approx" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from variational posterior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>Approximation to sample from</p>
</dd>
<dt><strong>draws: `int`</strong></dt><dd><p>Number of random samples.</p>
</dd>
<dt><strong>include_transformed: `bool`</strong></dt><dd><p>If True, transformed variables are also sampled. Default is True.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>trace: class:<cite>pymc3.backends.base.MultiTrace</cite></dt><dd><p>Samples drawn from variational posterior.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pymc3.variational.operators">
<span id="operators"></span><h3>Operators<a class="headerlink" href="#module-pymc3.variational.operators" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="pymc3.variational.operators.KL">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.operators.</code><code class="sig-name descname">KL</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">approx</span></em>, <em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KL" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Operator based on Kullback Leibler Divergence</strong></p>
<p>This operator constructs Evidence Lower Bound (ELBO) objective</p>
<div class="math notranslate nohighlight">
\[ELBO_\beta = \log p(D|\theta) - \beta KL(q||p)\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[KL[q(v)||p(v)] = \int q(v)\log\frac{q(v)}{p(v)}dv\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>Approximation used for inference</p>
</dd>
<dt><strong>beta: float</strong></dt><dd><p>Beta parameter for KL divergence, scales the regularization term.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="pymc3.variational.operators.KL.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KL.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator itself</p>
<div class="math notranslate nohighlight">
\[(O^{p,q}f_{\theta})(z)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>f: :class:`TestFunction` or None</strong></dt><dd><p>function that takes <cite>z = self.input</cite> and returns
same dimensional output</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>TensorVariable</dt><dd><p>symbolically applied operator</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="pymc3.variational.operators.KSD">
<em class="property">class </em><code class="sig-prename descclassname">pymc3.variational.operators.</code><code class="sig-name descname">KSD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">approx</span></em>, <em class="sig-param"><span class="n">temperature</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KSD" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Operator based on Kernelized Stein Discrepancy</strong></p>
<dl class="simple">
<dt>Input: A target distribution with density function <span class="math notranslate nohighlight">\(p(x)\)</span></dt><dd><p>and a set of initial particles <span class="math notranslate nohighlight">\(\{x^0_i\}^n_{i=1}\)</span></p>
</dd>
</dl>
<p>Output: A set of particles <span class="math notranslate nohighlight">\(\{x_i\}^n_{i=1}\)</span> that approximates the target distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}x_i^{l+1} \leftarrow \epsilon_l \hat{\phi}^{*}(x_i^l) \\
\hat{\phi}^{*}(x) = \frac{1}{n}\sum^{n}_{j=1}[k(x^l_j,x) \nabla_{x^l_j} logp(x^l_j)/temp +
\nabla_{x^l_j} k(x^l_j,x)]\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>approx: :class:`Approximation`</strong></dt><dd><p>Approximation used for inference</p>
</dd>
<dt><strong>temperature: float</strong></dt><dd><p>Temperature for Stein gradient</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Qiang Liu, Dilin Wang (2016)
Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm
arXiv:1608.04471</p></li>
</ul>
<dl class="py method">
<dt id="pymc3.variational.operators.KSD.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KSD.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator itself</p>
<div class="math notranslate nohighlight">
\[(O^{p,q}f_{\theta})(z)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>f: :class:`TestFunction` or None</strong></dt><dd><p>function that takes <cite>z = self.input</cite> and returns
same dimensional output</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>TensorVariable</dt><dd><p>symbolically applied operator</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pymc3.variational.operators.KSD.objective_class">
<code class="sig-name descname">objective_class</code><a class="headerlink" href="#pymc3.variational.operators.KSD.objective_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">pymc3.variational.operators.KSDObjective</span></code></p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">This page uses <a href="https://analytics.google.com/">
    Google Analytics</a> to collect statistics. You can disable it by blocking
    the JavaScript coming from www.google-analytics.com.
    <script>
      (function() {
        var ga = document.createElement('script');
        ga.src = ('https:' == document.location.protocol ?
                  'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        ga.setAttribute('async', 'true');
        document.documentElement.firstChild.appendChild(ga);
      })();
    </script>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 3.4.3.<br />
        </p>
    </div>
</div>
  </body>
</html>