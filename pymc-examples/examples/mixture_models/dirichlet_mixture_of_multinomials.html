
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dirichlet mixtures of multinomials &#8212; PyMC3 3.11.0 documentation</title>
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/default.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../../_static/highlight.min.js"></script>
    <script src="../../../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../../../_static/PyMC3.ico"/>
    <link rel="author" title="About these documents" href="../../../about.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-176578023-1']);
  _gaq.push(['_trackPageview']);
</script>
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../../../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../../../nb_examples/index.html" class="item">Examples</a> <a href="../../../learn.html" class="item">Books + Videos</a> <a href="../../../api.html" class="item">API</a> <a href="../../../developer_guide.html" class="item">Developer Guide</a> <a href="../../../about.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../../../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Dirichlet-mixtures-of-multinomials">
<h1>Dirichlet mixtures of multinomials<a class="headerlink" href="#Dirichlet-mixtures-of-multinomials" title="Permalink to this headline">¶</a></h1>
<p>This example notebook demonstrates the use of a Dirichlet mixture of multinomials (a.k.a <a class="reference external" href="https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution">Dirichlet-multinomial</a> or DM) to model categorical count data. Models like this one are important in a variety of areas, including natural language processing, ecology, bioinformatics, and more.</p>
<p>The Dirichlet-multinomial can be understood as draws from a <a class="reference external" href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial distribution</a> where each sample has a slightly different probability vector, which is itself drawn from a common <a class="reference external" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>. This contrasts with the Multinomial distribution, which assumes that all observations arise from a single fixed probability vector. This enables the Dirichlet-multinomial
to accommodate more variable (a.k.a, over-dispersed) count data than the Multinomial.</p>
<p>Other examples of over-dispersed count distributions are the <a class="reference external" href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">Beta-binomial</a> (which can be thought of as a special case of the DM) or the <a class="reference external" href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative binomial</a> distributions.</p>
<p>The DM is also an example of marginalizing a mixture distribution over its latent parameters. This notebook will demonstrate the performance benefits that come from taking that approach.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Simulation-data">
<h2>Simulation data<a class="headerlink" href="#Simulation-data" title="Permalink to this headline">¶</a></h2>
<p>Let us simulate some over-dispersed, categorical count data for this example.</p>
<p>Here we are simulating from the DM distribution itself, so it is perhaps tautological to fit that model, but rest assured that data like these really do appear in the counts of different: (1) <a class="reference external" href="https://doi.org/10.1145/1102351.1102420">words in text corpuses</a>, (2) <a class="reference external" href="https://doi.org/10.12688/f1000research.8900.2">types of RNA molecules in a cell</a>, (3) <a class="reference external" href="https://doi.org/10.2307/2981696">items purchased by shoppers</a>.</p>
<p>Here we will discuss a community ecology example, pretending that we have observed counts of <span class="math notranslate nohighlight">\(k=5\)</span> different tree species in <span class="math notranslate nohighlight">\(n=10\)</span> different forests.</p>
<p>Our simulation will produce a two-dimensional matrix of integers (counts) where each row, (zero-)indexed by <span class="math notranslate nohighlight">\(i \in (0...n-1)\)</span>, is an observation (different forest), and each column <span class="math notranslate nohighlight">\(j \in (0...k-1)\)</span> is a category (tree species). We’ll parameterize this distribution with three things: - <span class="math notranslate nohighlight">\(\mathrm{frac}\)</span> : the expected fraction of each species, a <span class="math notranslate nohighlight">\(k\)</span>-dimensional vector on the simplex (i.e. sums-to-one) - <span class="math notranslate nohighlight">\(\mathrm{total\_count}\)</span> : the total number of items tallied in
each observation, - <span class="math notranslate nohighlight">\(\mathrm{conc}\)</span> : the concentration, controlling the overdispersion of our data, where larger values result in our distribution more closely approximating the multinomial.</p>
<p>Here, and throughout this notebook, we’ve used a <a class="reference external" href="https://mc-stan.org/docs/2_26/stan-users-guide/reparameterizations.html#dirichlet-priors">convenient reparameterization</a> of the Dirichlet distribution from one to two parameters, <span class="math notranslate nohighlight">\(\alpha=\mathrm{conc} \times \mathrm{frac}\)</span>, as this fits our desired interpretation.</p>
<p>Each observation from the DM is simulated by: 1. first obtaining a value on the <span class="math notranslate nohighlight">\(k\)</span>-simplex simulated as <span class="math notranslate nohighlight">\(p_i \sim \mathrm{Dirichlet}(\alpha=\mathrm{conc} \times \mathrm{frac})\)</span>, 2. and then simulating <span class="math notranslate nohighlight">\(\mathrm{counts}_i \sim \mathrm{Multinomial}(\mathrm{total\_count}, p_i)\)</span>.</p>
<p>Notice that each observation gets its <em>own</em> latent parameter <span class="math notranslate nohighlight">\(p_i\)</span>, simulated independently from a common Dirichlet distribution.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">true_conc</span> <span class="o">=</span> <span class="mf">6.0</span>
<span class="n">true_frac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">])</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_frac</span><span class="p">)</span>  <span class="c1"># Number of different tree species observed</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of forests observed</span>
<span class="n">total_count</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">true_p</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">true_conc</span> <span class="o">*</span> <span class="n">true_frac</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">observed_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_i</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span> <span class="k">for</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="n">true_p</span><span class="p">])</span>

<span class="n">observed_counts</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[33,  8,  4,  1,  4],
       [22, 28,  0,  0,  0],
       [35, 11,  2,  2,  0],
       [32,  1,  7, 10,  0],
       [24, 22,  4,  0,  0],
       [28, 13,  9,  0,  0],
       [19,  4, 21,  6,  0],
       [26, 17,  1,  6,  0],
       [32, 16,  0,  2,  0],
       [10, 30,  5,  5,  0]])
</pre></div></div>
</div>
</div>
<div class="section" id="Multinomial-model">
<h2>Multinomial model<a class="headerlink" href="#Multinomial-model" title="Permalink to this headline">¶</a></h2>
<p>The first model that we will fit to these data is a plain multinomial model, where the only parameter is the expected fraction of each category, <span class="math notranslate nohighlight">\(\mathrm{frac}\)</span>, which we will give a Dirichlet prior. While the uniform prior (<span class="math notranslate nohighlight">\(\alpha_j=1\)</span> for each <span class="math notranslate nohighlight">\(j\)</span>) works well, if we have independent beliefs about the fraction of each tree, we could encode this into our prior, e.g. increasing the value of <span class="math notranslate nohighlight">\(\alpha_j\)</span> where we expect a higher fraction of species-<span class="math notranslate nohighlight">\(j\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_multinomial</span><span class="p">:</span>
    <span class="n">frac</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">frac</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed_counts</span><span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model_multinomial</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_9_0.svg" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_9_0.svg" /></div>
</div>
<p>Interestingly, NUTS frequently runs into numerical problems on this model, perhaps an example of the <a class="reference external" href="https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/">“Folk Theorem of Statistical Computing”</a>.</p>
<p>Because of a couple of identities of the multinomial distribution, we could reparameterize this model in a number of ways—we would obtain equivalent models by exploding our <span class="math notranslate nohighlight">\(n\)</span> observations of <span class="math notranslate nohighlight">\(\mathrm{total\_count}\)</span> items into <span class="math notranslate nohighlight">\((n \times \mathrm{total\_count})\)</span> independent categorical trials, or collapsing them down into one Multinomial draw with <span class="math notranslate nohighlight">\((n \times \mathrm{total\_count})\)</span> items. (Importantly, this is <em>not</em> true for the DM distribution.)</p>
<p>Rather than <em>actually</em> fixing our problem through reparameterization, here we’ll instead switch to the Metropolis step method, which ignores some of the geometric pathologies of our naïve model.</p>
<p><strong>Important</strong>: switching to Metropolis does not not <em>fix</em> our model’s issues, rather it <em>sweeps them under the rug</em>. In fact, if you try running this model with NUTS (PyMC3’s default step method), it will break loudly during sampling. When that happens, this should be a <strong>red alert</strong> that there is something wrong in our model.</p>
<p>You’ll also notice below that we have to increase considerably the number of draws we take from the posterior; this is because Metropolis is much less efficient at exploring the posterior than NUTS.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">model_multinomial</span><span class="p">:</span>
    <span class="n">trace_multinomial</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="n">draws</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">5e3</span><span class="p">),</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">(),</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Multiprocess sampling (4 chains in 2 jobs)
Metropolis: [frac]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value='24000' class='' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [24000/24000 00:07<00:00 Sampling 4 chains, 0 divergences]
</div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 18 seconds.
The number of effective samples is smaller than 10% for some parameters.
</pre></div></div>
</div>
<p>Let’s ignore the warning about inefficient sampling for now.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_multinomial</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_13_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_13_0.png" />
</div>
</div>
<p>The trace plots look fairly good; visually, each parameter appears to be moving around the posterior well, although some sharp parts of the KDE plot suggests that sampling sometimes gets stuck in one place for a few steps.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">summary_multinomial</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_multinomial</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">])</span>
<span class="n">summary_multinomial</span> <span class="o">=</span> <span class="n">summary_multinomial</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">ess_mean_per_sec</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">ess_mean</span> <span class="o">/</span> <span class="n">trace_multinomial</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">sampling_time</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">summary_multinomial</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
      <th>ess_mean_per_sec</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>frac[0]</th>
      <td>0.518</td>
      <td>0.022</td>
      <td>0.474</td>
      <td>0.556</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2020.0</td>
      <td>2015.0</td>
      <td>2028.0</td>
      <td>2516.0</td>
      <td>1.00</td>
      <td>110.249714</td>
    </tr>
    <tr>
      <th>frac[1]</th>
      <td>0.299</td>
      <td>0.021</td>
      <td>0.261</td>
      <td>0.338</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1941.0</td>
      <td>1941.0</td>
      <td>1938.0</td>
      <td>2310.0</td>
      <td>1.00</td>
      <td>105.937968</td>
    </tr>
    <tr>
      <th>frac[2]</th>
      <td>0.107</td>
      <td>0.014</td>
      <td>0.083</td>
      <td>0.133</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1259.0</td>
      <td>1259.0</td>
      <td>1257.0</td>
      <td>1729.0</td>
      <td>1.00</td>
      <td>68.715045</td>
    </tr>
    <tr>
      <th>frac[3]</th>
      <td>0.066</td>
      <td>0.011</td>
      <td>0.046</td>
      <td>0.087</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>767.0</td>
      <td>767.0</td>
      <td>734.0</td>
      <td>1260.0</td>
      <td>1.01</td>
      <td>41.862144</td>
    </tr>
    <tr>
      <th>frac[4]</th>
      <td>0.010</td>
      <td>0.005</td>
      <td>0.003</td>
      <td>0.019</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>516.0</td>
      <td>516.0</td>
      <td>457.0</td>
      <td>538.0</td>
      <td>1.01</td>
      <td>28.162798</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Likewise, diagnostics in the parameter summary table all look fine. Here I’ve added a column estimating the effective sample size per second of sampling.</p>
<p>Nonetheless, the fact that we were unable to use NUTS is still a red flag, and we should be very cautious in using these results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_multinomial</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">y_tick</span><span class="p">,</span> <span class="n">frac_j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_yticks</span><span class="p">(),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">true_frac</span><span class="p">))):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">frac_j</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">y_tick</span> <span class="o">-</span> <span class="mf">0.45</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">y_tick</span> <span class="o">+</span> <span class="mf">0.45</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_17_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_17_0.png" />
</div>
</div>
<p>Here we’ve drawn a forest-plot, showing the mean and 94% HDIs from our posterior approximation. Interestingly, because we know what the underlying frequencies are for each species (dashed lines), we can comment on the accuracy of our inferences. And now the issues with our model become apparent; notice that the 94% HDIs <em>don’t include the true values</em> for tree species 0, 2, 3. We might have seen <em>one</em> HDI miss, but <em>three</em>???</p>
<p>…what’s going on?</p>
<p>Let’s troubleshoot this model using a posterior-predictive check, comparing our data to simulated data conditioned on our posterior estimates.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">model_multinomial</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fast_sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_multinomial</span><span class="p">,</span>
        <span class="n">keep_size</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Concatenate with InferenceData object</span>
<span class="n">trace_multinomial</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">az</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">posterior_predictive</span><span class="o">=</span><span class="n">ppc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;tab10&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
        <span class="n">trace_multinomial</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="o">.</span><span class="n">counts</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">total_count</span><span class="p">),</span>
        <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
        <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Post.Pred.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
        <span class="p">(</span><span class="n">trace_multinomial</span><span class="o">.</span><span class="n">observed_data</span><span class="o">.</span><span class="n">counts</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
        <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">total_count</span><span class="p">),</span>
        <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
        <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observed&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">true_frac</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">total_count</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;species-</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
        <span class="n">xycoords</span><span class="o">=</span><span class="s2">&quot;axes fraction&quot;</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper center&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_20_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_20_0.png" />
</div>
</div>
<p>Here we’re plotting histograms of the predicted counts against the observed counts for each species.</p>
<p><em>(Notice that the y-axis isn’t full height and clips the distributions for species-4 in purple.)</em></p>
<p>And now we can start to see why our posterior HDI deviates from the <em>true</em> parameters for three of five species (vertical lines). See that for all of the species the observed counts are frequently quite far from the predictions conditioned on the posterior distribution. This is particularly obvious for (e.g.) species-2 where we have one observation of more than 20 trees of this species, despite the posterior predicitive mass being concentrated far below that.</p>
<p>This is overdispersion at work, and a clear sign that we need to adjust our model to accomodate it.</p>
<p>Posterior predictive checks are one of the best ways to diagnose model misspecification, and this example is no different.</p>
</div>
<div class="section" id="Dirichlet-Multinomial-Model---Explicit-Mixture">
<h2>Dirichlet-Multinomial Model - Explicit Mixture<a class="headerlink" href="#Dirichlet-Multinomial-Model---Explicit-Mixture" title="Permalink to this headline">¶</a></h2>
<p>Let’s go ahead and model our data using the DM distribution.</p>
<p>For this model we’ll keep the same prior on the expected frequencies of each species, <span class="math notranslate nohighlight">\(\mathrm{frac}\)</span>. We’ll also add a strictly positive parameter, <span class="math notranslate nohighlight">\(\mathrm{conc}\)</span>, for the concentration.</p>
<p>In this iteration of our model we’ll explicitly include the latent multinomial probability, <span class="math notranslate nohighlight">\(p_i\)</span>, modeling the <span class="math notranslate nohighlight">\(\mathrm{true\_p}_i\)</span> from our simulations (which we would not observe in the real world).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_dm_explicit</span><span class="p">:</span>
    <span class="n">frac</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="n">conc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Lognormal</span><span class="p">(</span><span class="s2">&quot;conc&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">frac</span> <span class="o">*</span> <span class="n">conc</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed_counts</span><span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model_dm_explicit</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_24_0.svg" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_24_0.svg" /></div>
</div>
<p>Compare this diagram to the first. Here the latent, Dirichlet distributed <span class="math notranslate nohighlight">\(p\)</span> separates the multinomial from the expected frequencies, <span class="math notranslate nohighlight">\(\mathrm{frac}\)</span>, accounting for overdispersion of counts relative to the simple multinomial model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">model_dm_explicit</span><span class="p">:</span>
    <span class="n">trace_dm_explicit</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 2 jobs)
NUTS: [p, conc, frac]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [8000/8000 02:47<00:00 Sampling 4 chains, 11 divergences]
</div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 182 seconds.
There were 3 divergences after tuning. Increase `target_accept` or reparameterize.
There was 1 divergence after tuning. Increase `target_accept` or reparameterize.
There were 7 divergences after tuning. Increase `target_accept` or reparameterize.
The acceptance probability does not match the target. It is 0.9041835811665464, but should be close to 0.8. Try to increase the number of tuning steps.
The estimated number of effective samples is smaller than 200 for some parameters.
</pre></div></div>
</div>
<p>We got a warning, although we’ll ignore it for now. More interesting is how much longer it took to sample this model than the first. This may be because our model has an additional ~<span class="math notranslate nohighlight">\((n \times k)\)</span> parameters, but it seems like there are other geometric challenges for NUTS as well.</p>
<p>We’ll see if we can fix these in the next model, but for now let’s take a look at the traces.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_dm_explicit</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="s2">&quot;conc&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_28_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_28_0.png" />
</div>
</div>
<p>Obviously some sampling issues, but it’s hard to see where divergences are occurring.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_dm_explicit</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">y_tick</span><span class="p">,</span> <span class="n">frac_j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_yticks</span><span class="p">(),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">true_frac</span><span class="p">))):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">frac_j</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">y_tick</span> <span class="o">-</span> <span class="mf">0.45</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">y_tick</span> <span class="o">+</span> <span class="mf">0.45</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_30_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_30_0.png" />
</div>
</div>
<p>On the other hand, since we know the ground-truth for <span class="math notranslate nohighlight">\(\mathrm{frac}\)</span>, we can congratulate ourselves that the HDIs include the true values for all of our species!</p>
<p>Modeling this mixture has made our inferences robust to the overdispersion of counts, while the plain multinomial is very sensitive. Notice that the HDI is much wider than before for each <span class="math notranslate nohighlight">\(\mathrm{frac}_i\)</span>. In this case that makes the difference between correct and incorrect inferences.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">summary_dm_explicit</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_dm_explicit</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="s2">&quot;conc&quot;</span><span class="p">])</span>
<span class="n">summary_dm_explicit</span> <span class="o">=</span> <span class="n">summary_dm_explicit</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">ess_mean_per_sec</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">ess_mean</span> <span class="o">/</span> <span class="n">trace_dm_explicit</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">sampling_time</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">summary_dm_explicit</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
      <th>ess_mean_per_sec</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>frac[0]</th>
      <td>0.499</td>
      <td>0.063</td>
      <td>0.378</td>
      <td>0.613</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>4058.0</td>
      <td>4058.0</td>
      <td>4115.0</td>
      <td>2871.0</td>
      <td>1.00</td>
      <td>22.319671</td>
    </tr>
    <tr>
      <th>frac[1]</th>
      <td>0.280</td>
      <td>0.053</td>
      <td>0.183</td>
      <td>0.379</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>4549.0</td>
      <td>4549.0</td>
      <td>4506.0</td>
      <td>2604.0</td>
      <td>1.00</td>
      <td>25.020252</td>
    </tr>
    <tr>
      <th>frac[2]</th>
      <td>0.117</td>
      <td>0.034</td>
      <td>0.057</td>
      <td>0.182</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>3236.0</td>
      <td>3236.0</td>
      <td>3184.0</td>
      <td>2919.0</td>
      <td>1.00</td>
      <td>17.798535</td>
    </tr>
    <tr>
      <th>frac[3]</th>
      <td>0.089</td>
      <td>0.030</td>
      <td>0.038</td>
      <td>0.144</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>2721.0</td>
      <td>2721.0</td>
      <td>2605.0</td>
      <td>2643.0</td>
      <td>1.00</td>
      <td>14.965950</td>
    </tr>
    <tr>
      <th>frac[4]</th>
      <td>0.015</td>
      <td>0.011</td>
      <td>0.001</td>
      <td>0.036</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>163.0</td>
      <td>163.0</td>
      <td>112.0</td>
      <td>120.0</td>
      <td>1.03</td>
      <td>0.896527</td>
    </tr>
    <tr>
      <th>conc</th>
      <td>6.143</td>
      <td>2.031</td>
      <td>2.739</td>
      <td>9.910</td>
      <td>0.047</td>
      <td>0.033</td>
      <td>1857.0</td>
      <td>1857.0</td>
      <td>1799.0</td>
      <td>2662.0</td>
      <td>1.00</td>
      <td>10.213807</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>This is great, but <em>we can do better</em>. The larger <span class="math notranslate nohighlight">\(\hat{R}\)</span> value for <span class="math notranslate nohighlight">\(\mathrm{frac}_4\)</span> is mildly concerning, and it’s surprising that our <span class="math notranslate nohighlight">\(\mathrm{ESS} \; \mathrm{sec}^{-1}\)</span> is relatively small.</p>
</div>
<div class="section" id="Dirichlet-Multinomial-Model---Marginalized">
<h2>Dirichlet-Multinomial Model - Marginalized<a class="headerlink" href="#Dirichlet-Multinomial-Model---Marginalized" title="Permalink to this headline">¶</a></h2>
<p>Happily, the Dirichlet distribution is conjugate to the multinomial and therefore there’s a convenient, closed-form for the marginalized distribution, i.e. the Dirichlet-multinomial distribution, which was added to PyMC3 in <a class="reference external" href="https://github.com/pymc-devs/pymc3/releases/tag/v3.11.0">3.11.0</a>.</p>
<p>Let’s take advantage of this, marginalizing out the explicit latent parameter, <span class="math notranslate nohighlight">\(p_i\)</span>, replacing the combination of this node and the multinomial with the DM to make an equivalent model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_dm_marginalized</span><span class="p">:</span>
    <span class="n">frac</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="n">conc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Lognormal</span><span class="p">(</span><span class="s2">&quot;conc&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">DirichletMultinomial</span><span class="p">(</span>
        <span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">frac</span> <span class="o">*</span> <span class="n">conc</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed_counts</span>
    <span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model_dm_marginalized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_36_0.svg" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_36_0.svg" /></div>
</div>
<p>The plate diagram shows that we’ve collapsed what had been the latent Dirichlet and the multinomial nodes together into a single DM node.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">model_dm_marginalized</span><span class="p">:</span>
    <span class="n">trace_dm_marginalized</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 2 jobs)
NUTS: [conc, frac]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [8000/8000 00:17<00:00 Sampling 4 chains, 0 divergences]
</div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 34 seconds.
</pre></div></div>
</div>
<p>It samples much more quickly and without any of the warnings from before!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_dm_marginalized</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="s2">&quot;conc&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_40_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_40_0.png" />
</div>
</div>
<p>Trace plots look fuzzy and KDEs are clean.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">summary_dm_marginalized</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_dm_marginalized</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;frac&quot;</span><span class="p">,</span> <span class="s2">&quot;conc&quot;</span><span class="p">])</span>
<span class="n">summary_dm_marginalized</span> <span class="o">=</span> <span class="n">summary_dm_marginalized</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">ess_mean_per_sec</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">ess_mean</span> <span class="o">/</span> <span class="n">trace_dm_marginalized</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">sampling_time</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">summary_dm_marginalized</span><span class="o">.</span><span class="n">r_hat</span> <span class="o">&lt;</span> <span class="mf">1.03</span><span class="p">)</span>

<span class="n">summary_dm_marginalized</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
      <th>ess_mean_per_sec</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>frac[0]</th>
      <td>0.500</td>
      <td>0.063</td>
      <td>0.388</td>
      <td>0.621</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>4543.0</td>
      <td>4543.0</td>
      <td>4609.0</td>
      <td>2932.0</td>
      <td>1.0</td>
      <td>133.853339</td>
    </tr>
    <tr>
      <th>frac[1]</th>
      <td>0.282</td>
      <td>0.054</td>
      <td>0.177</td>
      <td>0.381</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>6048.0</td>
      <td>5875.0</td>
      <td>6022.0</td>
      <td>2937.0</td>
      <td>1.0</td>
      <td>178.196124</td>
    </tr>
    <tr>
      <th>frac[2]</th>
      <td>0.116</td>
      <td>0.035</td>
      <td>0.057</td>
      <td>0.183</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>4317.0</td>
      <td>4275.0</td>
      <td>4229.0</td>
      <td>3243.0</td>
      <td>1.0</td>
      <td>127.194555</td>
    </tr>
    <tr>
      <th>frac[3]</th>
      <td>0.087</td>
      <td>0.029</td>
      <td>0.035</td>
      <td>0.143</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>2897.0</td>
      <td>2897.0</td>
      <td>2791.0</td>
      <td>2580.0</td>
      <td>1.0</td>
      <td>85.356179</td>
    </tr>
    <tr>
      <th>frac[4]</th>
      <td>0.015</td>
      <td>0.011</td>
      <td>0.000</td>
      <td>0.034</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3064.0</td>
      <td>2898.0</td>
      <td>2685.0</td>
      <td>2072.0</td>
      <td>1.0</td>
      <td>90.276608</td>
    </tr>
    <tr>
      <th>conc</th>
      <td>6.213</td>
      <td>2.032</td>
      <td>2.692</td>
      <td>9.812</td>
      <td>0.037</td>
      <td>0.027</td>
      <td>3017.0</td>
      <td>2866.0</td>
      <td>3063.0</td>
      <td>3303.0</td>
      <td>1.0</td>
      <td>88.891817</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We see that <span class="math notranslate nohighlight">\(\hat{R}\)</span> is close to <span class="math notranslate nohighlight">\(1\)</span> everywhere and <span class="math notranslate nohighlight">\(\mathrm{ESS} \; \mathrm{sec}^{-1}\)</span> is much higher. Our reparameterization (marginalization) has greatly improved the sampling! (And, thankfully, the HDIs look similar to the other model.)</p>
<p>This all looks very good, but what if we didn’t have the ground-truth?</p>
<p>Posterior predictive checks to the rescue (again)!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">model_dm_marginalized</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fast_sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_dm_marginalized</span><span class="p">,</span> <span class="n">keep_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Concatenate with InferenceData object</span>
<span class="n">trace_dm_marginalized</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">az</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">posterior_predictive</span><span class="o">=</span><span class="n">ppc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;tab10&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_trace</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">trace_dm_marginalized</span><span class="p">,</span> <span class="n">trace_multinomial</span><span class="p">],</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
            <span class="n">_trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="o">.</span><span class="n">counts</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
            <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">total_count</span><span class="p">),</span>
            <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
            <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Post.Pred.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
            <span class="p">(</span><span class="n">_trace</span><span class="o">.</span><span class="n">observed_data</span><span class="o">.</span><span class="n">counts</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
            <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">total_count</span><span class="p">),</span>
            <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
            <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observed&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
            <span class="n">true_frac</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">total_count</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
            <span class="n">lw</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;species-</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
        <span class="n">xycoords</span><span class="o">=</span><span class="s2">&quot;axes fraction&quot;</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper center&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Multinomial&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dirichlet-multinomial&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_45_0.png" src="../../../_images/pymc-examples_examples_mixture_models_dirichlet_mixture_of_multinomials_45_0.png" />
</div>
</div>
<p><em>(Notice, again, that the y-axis isn’t full height, and clips the distributions for species-4 in purple.)</em></p>
<p>Compared to the multinomial (plots on the right), PPCs for the DM (left) show that the observed data is an entirely reasonable realization of our model. This is great news!</p>
</div>
<div class="section" id="Model-Comparison">
<h2>Model Comparison<a class="headerlink" href="#Model-Comparison" title="Permalink to this headline">¶</a></h2>
<p>Let’s go a step further and try to put a number on how much better our DM model is relative to the raw multinomial. We’ll use leave-one-out cross validation to compare the out-of-sample predictive ability of the two.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;multinomial&quot;</span><span class="p">:</span> <span class="n">trace_multinomial</span><span class="p">,</span> <span class="s2">&quot;dirichlet_multinomial&quot;</span><span class="p">:</span> <span class="n">trace_dm_marginalized</span><span class="p">},</span> <span class="n">ic</span><span class="o">=</span><span class="s2">&quot;loo&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/byronsmith/anaconda3/envs/pymc3-dev/lib/python3.9/site-packages/arviz/stats/stats.py:146: UserWarning: The default method used to estimate the weights for each model,has changed from BB-pseudo-BMA to stacking
  warnings.warn(
/Users/byronsmith/anaconda3/envs/pymc3-dev/lib/python3.9/site-packages/arviz/stats/stats.py:913: RuntimeWarning: overflow encountered in exp
  weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)
/Users/byronsmith/anaconda3/envs/pymc3-dev/lib/python3.9/site-packages/arviz/stats/stats.py:692: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>loo</th>
      <th>p_loo</th>
      <th>d_loo</th>
      <th>weight</th>
      <th>se</th>
      <th>dse</th>
      <th>warning</th>
      <th>loo_scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>dirichlet_multinomial</th>
      <td>0</td>
      <td>-96.382639</td>
      <td>4.322324</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>5.861086</td>
      <td>0.000000</td>
      <td>False</td>
      <td>log</td>
    </tr>
    <tr>
      <th>multinomial</th>
      <td>1</td>
      <td>-161.543594</td>
      <td>24.431986</td>
      <td>65.160955</td>
      <td>0.0</td>
      <td>22.336271</td>
      <td>18.207668</td>
      <td>True</td>
      <td>log</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Unsurprisingly, the DM outclasses the multinomial by a mile, assigning a weight of nearly 100% to the over-dispersed model. We can conclude that between the two, the DM should be greatly favored for prediction, parameter inference, etc.</p>
</div>
<div class="section" id="Conclusions">
<h2>Conclusions<a class="headerlink" href="#Conclusions" title="Permalink to this headline">¶</a></h2>
<p>Obviously the DM is not a perfect model in every case, but it is often a better choice than the multinomial, much more robust while taking on just one additional parameter.</p>
<p>There are a number of shortcomings to the DM that we should keep in mind when selecting a model. The biggest problem is that, while more flexible than the multinomial, the DM still ignores the possibility of underlying correlations between categories. If one of our tree species relies on another, for instance, the model we’ve used here will not effectively account for this. In that case, swapping the vanilla Dirichlet distribution for something fancier (e.g. the <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_Dirichlet_distribution">Generalized
Dirichlet</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Logit-normal_distribution#Multivariate_generalization">Logistic-Multivariate Normal</a>) may be worth considering.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Last updated: Mon Jan 25 2021

Python implementation: CPython
Python version       : 3.9.1
IPython version      : 7.19.0

scipy     : 1.6.0
seaborn   : 0.11.1
pymc3     : 3.10.0
json      : 2.0.9
numpy     : 1.19.4
matplotlib: 3.3.3
arviz     : 0.11.0

Watermark: 2.1.0

</pre></div></div>
</div>
</div>
</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">This page uses <a href="https://analytics.google.com/">
    Google Analytics</a> to collect statistics. You can disable it by blocking
    the JavaScript coming from www.google-analytics.com.
    <script>
      (function() {
        var ga = document.createElement('script');
        ga.src = ('https:' == document.location.protocol ?
                  'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        ga.setAttribute('async', 'true');
        document.documentElement.firstChild.appendChild(ga);
      })();
    </script>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 3.4.3.<br />
        </p>
    </div>
</div>
  </body>
</html>